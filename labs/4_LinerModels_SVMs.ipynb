{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.3 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
        }
      }
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3-final"
    },
    "nav_menu": {},
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "CMT316_Lab4_LinerModels_SVMs.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn-5RznvUSvI"
      },
      "source": [
        "**CMT316 Lab 4 â€“ Linear Models & Support Vector Machines**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nodTq9sFUSvR"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHU_cia1USvV"
      },
      "source": [
        "Import a few common modules, ensure MatplotLib plots figures inline:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s87jJctlUSvX"
      },
      "source": [
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Ignore useless warnings (see SciPy issue #5998)\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCaRVL-IUSvh"
      },
      "source": [
        "# Linear regression\n",
        "## Linear regression using the Normal Equation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs7NhPK7USvj"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "X = 2 * np.random.rand(100, 1)    # uniformly generate 100 random numbers, i.e., 100 rows and 1 column. X in [0, 2)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)   # a linear function y = 4 + 3 * X, plus Gaussian noise N(0, 1) (normal distribution)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZQ7cFSEzq4z"
      },
      "source": [
        "Plot the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY-a_NeXUSvt"
      },
      "source": [
        "plt.plot(X, y, \"b.\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.axis([0, 2, 0, 15])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbFLgRr60Vjb"
      },
      "source": [
        "Find model parameters using normal equation ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIIAAAAYCAYAAAA2/iXYAAAEiElEQVRoBe1YvS9sQRQ/u3kl1TZEFLbYUoJQCwrCv4CGVRLrD9D7iEIjlEKhZEWBCI2IjyjZmtUohEI3L79Jzs3Zu3PvnXvv7vLe3ZNM5uvM+Z6ZM5NSSilqQuItkE68BZoG0Bb487/b4evri66urqrUbGtro56enqrxRgw8PDxQe3s7QYawUC99/vsT4eLignZ2dujl5YUmJyd1jf75+XlYH8TGL5VKlM/nqbe3l25vbyPRq5c+gYHAwqdSKa0E+j8Bx8fHNDY2RthNYSCdTtPm5iZ9fHzoZX19fTQ/P09DQ0NhyGjctbW1yDYol8t0eXkZia8UtJb6SLqEZNEL9vf3VS6XU+vr66pcLivuo24krK6uqsXFRfX5+emwRXt4eBiJri6ZTEbd39/rgjbGMc9rpqamVDabVa+vrw4NNMLSeX5+VqOjo5pPBSHLzuHhoZYNtYSwcnjpI2mGaRsDAULNzs5qx7MhJVEEwtLSkmNkOVfrNgwmHSrps1HhdAQLAI6Gw/v7+xWcBmAjw3gmsKXDaxFwoM9BBT6wRz6fryrFYpGX6Zp5oXYDz8XVx03Xpm8MBJuFjcBhB5qMBv48D8Oxk1FLJwEPjsMpwcHilt2WjlwHPpJeqVRSpgLaEtjZJp1s5QjSR/KzbRsDAQrCuCgsMO803p0sjMSxZWqL5955pnVsWJwChULBePwzDusSlY5cB1qmq0bimNpBsvB8HH22t7d14MM3HKzsL75C3bIZA0FGJhtPCshHInYFB4YkLNdzQJlq01pJB0oE4UheJsfIeb/cRuKZ6Ei50IZhkT+htgU4CKcVbIEafTcEySHnTfpAHtDFHPhIXbz8BRmM/wgtLS3U0dGhk8qnpyeamJigg4ODiiQT79nv729aWVkh4EtA//T0VA7VrS1l7erqotbW1gpetrIE0akgGrEzMzNDKH4QJEeQPvgbQcErJZvN6tfS29ubtgue0AsLC1X+gjyez8fu7m5H3qOjIyoWi5TL5RzCW1tbNDAw8GOfMixcoVCg6+trymQydHZ2RnhnR4Fa0YnCW66plRzYENgY7+/v+u+E7TI4OCjZOW3PQGCMk5MT/e7GW7yzs1MP7+3t0c3NDc3NzTFaRY3TYmRkhPD34FeAA9yoMD09TY+Pj3R3d0fj4+OazMbGRmiataITVQ9eV0s5cHLg3wWwu7tLCDDT6c28jTkC7gzOCXDPcMKBO8Z977jvuFr2cd+5XwBM333fAReJkFcyxOvcdVQ6sI+8f910w/ajyuHHR/oQbT/wDAQ2LAREggLg10QQUT+GYeY4MZL8kKgigeSEixPX5eVlPcbj/IfgxS8uHdgFJS7ElcOPv/ShHx7mPAMhaGGj5hEEQS+HRsnCfPxOKsb5DbXNq4vl/PWBAEGhkPuLmRVodB33i7ne8uKUwubB8xFf4XxiBvH9JwIBSuCrNs4ff5AhbOYRkPh6D7p2bGjVA4evUlyPkJOvdBteKSA5mWOzkVgLBD4fE2uZhCneDISEOdxL3WYgeFkmYePNQEiYw73UbQaCl2USNv4XOibNjTdXUqEAAAAASUVORK5CYII=)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWG-pqVrUSv_"
      },
      "source": [
        "X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance, this introduces a constant feature to incorperate b=w0 into parameter vector\n",
        "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)    #  theta_best is w in above normal equation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-i1igS89_6X"
      },
      "source": [
        "Note we generated data based on y = 4 + 3 * X, i.e., **w_0 = 4, w_1 = 3**. Comparison the calculated parameters below and actual values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11x4vMN4USwH"
      },
      "source": [
        "theta_best  # print model parameters. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FlXBhPX4hT_"
      },
      "source": [
        "Using the learning model to predict y values for two given values of x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6AQOZafUSwP"
      },
      "source": [
        "X_new = np.array([[0], [2]])             # two x values\n",
        "X_new_b = np.c_[np.ones((2, 1)), X_new]  # add constant feature x0 = 1 to each instance\n",
        "y_predict = X_new_b.dot(theta_best)      # make prediction using the learning model y = Xw\n",
        "y_predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm2wx0ia5Gv4"
      },
      "source": [
        "Plot the model (red line) along with the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y7i8V31USwk"
      },
      "source": [
        "plt.plot(X_new, y_predict, \"r-\", linewidth=2, label=\"Predictions\")\n",
        "plt.plot(X, y, \"b.\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.legend(loc=\"upper left\", fontsize=14)\n",
        "plt.axis([0, 2, 0, 15])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCeqxu986NwN"
      },
      "source": [
        "## Linear regression using **LinearRegression** of sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_gAIRslUSwt"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X, y)                   # note that it takes X instead of X_b, as it treats the intercept b as a separate parameter\n",
        "lin_reg.intercept_, lin_reg.coef_   # show model parameters. Note intercept_ is w0, coef_ are other parameters, i.e., w1 in this example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOHBsH9QUSw0"
      },
      "source": [
        "lin_reg.predict(X_new)  # predict for the given x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPtY6CRj9LeN"
      },
      "source": [
        "## Exercise\n",
        "Generate 100 examples using a linear function with 3 features/variables, add a Gaussian noise *N*(0,1) to it. For example, \n",
        "y = 4 + 2*x_1 + 3* x_2 + 4 x_3 + N(0,1). Train a linear model to fit the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md-swZTegHMk"
      },
      "source": [
        "## Linear regression through Stochastic Gradient Descent using SGDRegressor of sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf7byjg_USyu"
      },
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "sgd_reg = SGDRegressor(max_iter=50, tol=-np.infty, penalty=None, eta0=0.1, random_state=42)\n",
        "sgd_reg.fit(X, y.ravel())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSXN1LzpUSy1"
      },
      "source": [
        "sgd_reg.intercept_, sgd_reg.coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FETfMfUQUSzi"
      },
      "source": [
        "## Polynomial regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkHQXts0USzk"
      },
      "source": [
        "import numpy as np\n",
        "import numpy.random as rnd\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_nF74JAUSzq"
      },
      "source": [
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3                # generate 100 values between -3 and 3\n",
        "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)  # a quadratic function plus Gaussian noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGVfpOy2USz2"
      },
      "source": [
        "plt.plot(X, y, \"b.\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.axis([-3, 3, 0, 10])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bjcns-XYUSz-"
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)  # create a 2nd degree feature\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "X[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M-GyQ1LUS0E"
      },
      "source": [
        "X_poly[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL3TcVOaUS0K"
      },
      "source": [
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_poly, y)\n",
        "lin_reg.intercept_, lin_reg.coef_   # print the learned model parameters. note the original function y = 0.5 * X**2 + X + 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrZSmmTD6c6Z"
      },
      "source": [
        "Plot the learned prediction model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af6o_PqIUS0Q"
      },
      "source": [
        "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
        "X_new_poly = poly_features.transform(X_new)\n",
        "y_new = lin_reg.predict(X_new_poly)\n",
        "plt.plot(X, y, \"b.\")\n",
        "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.legend(loc=\"upper left\", fontsize=14)\n",
        "plt.axis([-3, 3, 0, 10])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHdhLmQO7GAE"
      },
      "source": [
        "## Illustration of overfitting and underfitting through different polynomial degrees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzXVo7-VUS0Y"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "for style, width, degree in ((\"g-\", 1, 300), (\"b--\", 2, 2), (\"r-+\", 2, 1)):\n",
        "    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    std_scaler = StandardScaler()\n",
        "    lin_reg = LinearRegression()\n",
        "    polynomial_regression = Pipeline([\n",
        "            (\"poly_features\", polybig_features),\n",
        "            (\"std_scaler\", std_scaler),\n",
        "            (\"lin_reg\", lin_reg),\n",
        "        ])\n",
        "    polynomial_regression.fit(X, y)\n",
        "    y_newbig = polynomial_regression.predict(X_new)\n",
        "    plt.plot(X_new, y_newbig, style, label='degree '+str(degree), linewidth=width)\n",
        "\n",
        "plt.plot(X, y, \"b.\", linewidth=3)\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.axis([-3, 3, 0, 10])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suleXh6uUS0w"
      },
      "source": [
        "## Regularized models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPqut6MPE0rJ"
      },
      "source": [
        "### Ridge regression\n",
        "Demonstration of **ridge regression** with and without polynomial terms, and with different regularization strength"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFcgqUKGUS0y"
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "np.random.seed(42)\n",
        "m = 20\n",
        "X = 3 * np.random.rand(m, 1)\n",
        "y = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5\n",
        "X_new = np.linspace(0, 3, 100).reshape(100, 1)\n",
        "\n",
        "def plot_model(model_class, polynomial, alphas, **model_kargs):\n",
        "    for alpha, style in zip(alphas, (\"b-\", \"g--\", \"r:\")):\n",
        "        model = model_class(alpha, **model_kargs) if alpha > 0 else LinearRegression()\n",
        "        if polynomial:\n",
        "            model = Pipeline([\n",
        "                    (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
        "                    (\"std_scaler\", StandardScaler()),\n",
        "                    (\"regul_reg\", model),\n",
        "                ])\n",
        "        model.fit(X, y)\n",
        "        y_new_regul = model.predict(X_new)\n",
        "        lw = 2 if alpha > 0 else 1\n",
        "        plt.plot(X_new, y_new_regul, style, linewidth=lw, label=r\"$\\alpha = {}$\".format(alpha))\n",
        "    plt.plot(X, y, \"b.\", linewidth=3)\n",
        "    plt.legend(loc=\"upper left\", fontsize=15)\n",
        "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "    plt.axis([0, 3, 0, 4])\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(121)\n",
        "plot_model(Ridge, polynomial=False, alphas=(0, 10, 100), random_state=42)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.subplot(122)\n",
        "plot_model(Ridge, polynomial=True, alphas=(0, 10**-5, 1), random_state=42)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9JsBtYUHX9d"
      },
      "source": [
        "Ridge regression using \"cholesky\" solver, which is the standard scipy.linalg.solve function to obtain a closed-form solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7L1nZjlUS05"
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "ridge_reg = Ridge(alpha=1, solver=\"cholesky\", random_state=42)\n",
        "ridge_reg.fit(X, y)\n",
        "ridge_reg.predict([[1.5]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTUKWWzDUS1A"
      },
      "source": [
        "sgd_reg = SGDRegressor(max_iter=50, tol=-np.infty, penalty=\"l2\", random_state=42)\n",
        "sgd_reg.fit(X, y.ravel())\n",
        "sgd_reg.predict([[1.5]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do_qcftmIgau"
      },
      "source": [
        "Uses a Stochastic Average Gradient descent \"sag\" solver."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuXgy4vZUS1M"
      },
      "source": [
        "ridge_reg = Ridge(alpha=1, solver=\"sag\", random_state=42)\n",
        "ridge_reg.fit(X, y)\n",
        "ridge_reg.predict([[1.5]])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQqZ7LLGJEED"
      },
      "source": [
        "### lasso\n",
        "Demonstration of **lasso** with and without polynomial terms, and with different regularization strength"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrz773JnUS1a"
      },
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(121)\n",
        "plot_model(Lasso, polynomial=False, alphas=(0, 0.1, 1), random_state=42)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.subplot(122)\n",
        "plot_model(Lasso, polynomial=True, alphas=(0, 10**-7, 1), tol=1, random_state=42)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2JcfdiXUS1i"
      },
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "lasso_reg = Lasso(alpha=0.1)\n",
        "lasso_reg.fit(X, y)\n",
        "lasso_reg.predict([[1.5]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqB4VjLIKJqa"
      },
      "source": [
        "### Elastic Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCrAIZ3tUS1o"
      },
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
        "elastic_net.fit(X, y)\n",
        "elastic_net.predict([[1.5]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbd8RQJVK_lX"
      },
      "source": [
        "### Demonstration of **early stopping**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7QZewKAzUS1z"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "np.random.seed(42)\n",
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X[:50], y[:50].ravel(), test_size=0.5, random_state=10)\n",
        "\n",
        "poly_scaler = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
        "        (\"std_scaler\", StandardScaler()),\n",
        "    ])\n",
        "\n",
        "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
        "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
        "\n",
        "sgd_reg = SGDRegressor(max_iter=1,\n",
        "                       tol=-np.infty,\n",
        "                       penalty=None,\n",
        "                       eta0=0.0005,\n",
        "                       warm_start=True,\n",
        "                       learning_rate=\"constant\",\n",
        "                       random_state=42)\n",
        "\n",
        "n_epochs = 500\n",
        "train_errors, val_errors = [], []\n",
        "for epoch in range(n_epochs):\n",
        "    sgd_reg.fit(X_train_poly_scaled, y_train)\n",
        "    y_train_predict = sgd_reg.predict(X_train_poly_scaled)\n",
        "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
        "    train_errors.append(mean_squared_error(y_train, y_train_predict))\n",
        "    val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
        "\n",
        "best_epoch = np.argmin(val_errors)\n",
        "best_val_rmse = np.sqrt(val_errors[best_epoch])\n",
        "\n",
        "plt.annotate('Best model',\n",
        "             xy=(best_epoch, best_val_rmse),\n",
        "             xytext=(best_epoch, best_val_rmse + 1),\n",
        "             ha=\"center\",\n",
        "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
        "             fontsize=16,\n",
        "            )\n",
        "\n",
        "best_val_rmse -= 0.03  # just to make the graph look better\n",
        "plt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], \"k:\", linewidth=2)\n",
        "plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"Validation set\")\n",
        "plt.plot(np.sqrt(train_errors), \"r--\", linewidth=2, label=\"Training set\")\n",
        "plt.legend(loc=\"upper right\", fontsize=14)\n",
        "plt.xlabel(\"Epoch\", fontsize=14)\n",
        "plt.ylabel(\"RMSE\", fontsize=14)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QeteKuQM5-p"
      },
      "source": [
        "Code to save the best model as training progress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrMGZoWhUS15"
      },
      "source": [
        "from sklearn.base import clone\n",
        "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True, penalty=None,\n",
        "                       learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
        "\n",
        "minimum_val_error = float(\"inf\")\n",
        "best_epoch = None\n",
        "best_model = None\n",
        "for epoch in range(1000):\n",
        "    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off\n",
        "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
        "    val_error = mean_squared_error(y_val, y_val_predict)\n",
        "    if val_error < minimum_val_error:\n",
        "        minimum_val_error = val_error\n",
        "        best_epoch = epoch\n",
        "        best_model = clone(sgd_reg)   # store the best model in best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qad7dw2rUS1-"
      },
      "source": [
        "best_epoch, best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm31E0j4PFwM"
      },
      "source": [
        "Illustration of parameter trajectory of Lasso versus Ridge regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx6CKn9GUS2E"
      },
      "source": [
        "t1a, t1b, t2a, t2b = -1, 3, -1.5, 1.5\n",
        "\n",
        "# ignoring bias term\n",
        "t1s = np.linspace(t1a, t1b, 500)\n",
        "t2s = np.linspace(t2a, t2b, 500)\n",
        "t1, t2 = np.meshgrid(t1s, t2s)\n",
        "T = np.c_[t1.ravel(), t2.ravel()]\n",
        "Xr = np.array([[-1, 1], [-0.3, -1], [1, 0.1]])\n",
        "yr = 2 * Xr[:, :1] + 0.5 * Xr[:, 1:]\n",
        "\n",
        "J = (1/len(Xr) * np.sum((T.dot(Xr.T) - yr.T)**2, axis=1)).reshape(t1.shape)\n",
        "\n",
        "N1 = np.linalg.norm(T, ord=1, axis=1).reshape(t1.shape)\n",
        "N2 = np.linalg.norm(T, ord=2, axis=1).reshape(t1.shape)\n",
        "\n",
        "t_min_idx = np.unravel_index(np.argmin(J), J.shape)\n",
        "t1_min, t2_min = t1[t_min_idx], t2[t_min_idx]\n",
        "\n",
        "t_init = np.array([[0.25], [-1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u30gJsrUS2M"
      },
      "source": [
        "def bgd_path(theta, X, y, l1, l2, core = 1, eta = 0.1, n_iterations = 50):\n",
        "    path = [theta]\n",
        "    for iteration in range(n_iterations):\n",
        "        gradients = core * 2/len(X) * X.T.dot(X.dot(theta) - y) + l1 * np.sign(theta) + 2 * l2 * theta\n",
        "\n",
        "        theta = theta - eta * gradients\n",
        "        path.append(theta)\n",
        "    return np.array(path)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, N, l1, l2, title in ((0, N1, 0.5, 0, \"Lasso\"), (1, N2, 0,  0.1, \"Ridge\")):\n",
        "    JR = J + l1 * N1 + l2 * N2**2\n",
        "    \n",
        "    tr_min_idx = np.unravel_index(np.argmin(JR), JR.shape)\n",
        "    t1r_min, t2r_min = t1[tr_min_idx], t2[tr_min_idx]\n",
        "\n",
        "    levelsJ=(np.exp(np.linspace(0, 1, 20)) - 1) * (np.max(J) - np.min(J)) + np.min(J)\n",
        "    levelsJR=(np.exp(np.linspace(0, 1, 20)) - 1) * (np.max(JR) - np.min(JR)) + np.min(JR)\n",
        "    levelsN=np.linspace(0, np.max(N), 10)\n",
        "    \n",
        "    path_J = bgd_path(t_init, Xr, yr, l1=0, l2=0)\n",
        "    path_JR = bgd_path(t_init, Xr, yr, l1, l2)\n",
        "    path_N = bgd_path(t_init, Xr, yr, np.sign(l1)/3, np.sign(l2), core=0)\n",
        "\n",
        "    plt.subplot(221 + i * 2)\n",
        "    plt.grid(True)\n",
        "    plt.axhline(y=0, color='k')\n",
        "    plt.axvline(x=0, color='k')\n",
        "    plt.contourf(t1, t2, J, levels=levelsJ, alpha=0.9)\n",
        "    plt.contour(t1, t2, N, levels=levelsN)\n",
        "    plt.plot(path_J[:, 0], path_J[:, 1], \"w-o\")\n",
        "    plt.plot(path_N[:, 0], path_N[:, 1], \"y-^\")\n",
        "    plt.plot(t1_min, t2_min, \"rs\")\n",
        "    plt.title(r\"$\\ell_{}$ penalty\".format(i + 1), fontsize=16)\n",
        "    plt.axis([t1a, t1b, t2a, t2b])\n",
        "    if i == 1:\n",
        "        plt.xlabel(r\"$\\theta_1$\", fontsize=20)\n",
        "    plt.ylabel(r\"$\\theta_2$\", fontsize=20, rotation=0)\n",
        "\n",
        "    plt.subplot(222 + i * 2)\n",
        "    plt.grid(True)\n",
        "    plt.axhline(y=0, color='k')\n",
        "    plt.axvline(x=0, color='k')\n",
        "    plt.contourf(t1, t2, JR, levels=levelsJR, alpha=0.9)\n",
        "    plt.plot(path_JR[:, 0], path_JR[:, 1], \"w-o\")\n",
        "    plt.plot(t1r_min, t2r_min, \"rs\")\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.axis([t1a, t1b, t2a, t2b])\n",
        "    if i == 1:\n",
        "        plt.xlabel(r\"$\\theta_1$\", fontsize=20)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhnlV-gwUS2R"
      },
      "source": [
        "# Logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EakhsFsnPgaF"
      },
      "source": [
        "Demontration of classification of Iris data using **logistic regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bu3HiagUS2Y"
      },
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "list(iris.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB6GDZGOUS2d"
      },
      "source": [
        "print(iris.DESCR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVREM4FyUS3P"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y = (iris[\"target\"] == 2).astype(np.int)\n",
        "\n",
        "log_reg = LogisticRegression(solver=\"liblinear\", C=10**10, random_state=42)\n",
        "log_reg.fit(X, y)\n",
        "\n",
        "x0, x1 = np.meshgrid(\n",
        "        np.linspace(2.9, 7, 500).reshape(-1, 1),\n",
        "        np.linspace(0.8, 2.7, 200).reshape(-1, 1),\n",
        "    )\n",
        "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
        "\n",
        "y_proba = log_reg.predict_proba(X_new)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\")\n",
        "plt.plot(X[y==1, 0], X[y==1, 1], \"g^\")\n",
        "\n",
        "zz = y_proba[:, 1].reshape(x0.shape)\n",
        "contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\n",
        "\n",
        "\n",
        "left_right = np.array([2.9, 7])\n",
        "boundary = -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) / log_reg.coef_[0][1]\n",
        "\n",
        "plt.clabel(contour, inline=1, fontsize=12)\n",
        "plt.plot(left_right, boundary, \"k--\", linewidth=3)\n",
        "plt.text(3.5, 1.5, \"Not Iris-Virginica\", fontsize=14, color=\"b\", ha=\"center\")\n",
        "plt.text(6.5, 2.3, \"Iris-Virginica\", fontsize=14, color=\"g\", ha=\"center\")\n",
        "plt.xlabel(\"Petal length\", fontsize=14)\n",
        "plt.ylabel(\"Petal width\", fontsize=14)\n",
        "plt.axis([2.9, 7, 0.8, 2.7])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUXgQwjvUS3f"
      },
      "source": [
        "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y = iris[\"target\"]\n",
        "\n",
        "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10, random_state=42)\n",
        "softmax_reg.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMo9M2XWUS3k"
      },
      "source": [
        "x0, x1 = np.meshgrid(\n",
        "        np.linspace(0, 8, 500).reshape(-1, 1),\n",
        "        np.linspace(0, 3.5, 200).reshape(-1, 1),\n",
        "    )\n",
        "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
        "\n",
        "\n",
        "y_proba = softmax_reg.predict_proba(X_new)\n",
        "y_predict = softmax_reg.predict(X_new)\n",
        "\n",
        "zz1 = y_proba[:, 1].reshape(x0.shape)\n",
        "zz = y_predict.reshape(x0.shape)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(X[y==2, 0], X[y==2, 1], \"g^\", label=\"Iris-Virginica\")\n",
        "plt.plot(X[y==1, 0], X[y==1, 1], \"bs\", label=\"Iris-Versicolor\")\n",
        "plt.plot(X[y==0, 0], X[y==0, 1], \"yo\", label=\"Iris-Setosa\")\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
        "\n",
        "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
        "contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\n",
        "plt.clabel(contour, inline=1, fontsize=12)\n",
        "plt.xlabel(\"Petal length\", fontsize=14)\n",
        "plt.ylabel(\"Petal width\", fontsize=14)\n",
        "plt.legend(loc=\"center left\", fontsize=14)\n",
        "plt.axis([0, 7, 0, 3.5])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLxZjfpRWSsN"
      },
      "source": [
        "# Support Vector Machines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vy71id5WSsN"
      },
      "source": [
        "## SVM for classification\n",
        "Functions for visualisation of data and decision boundary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRE4MwSjWSsO"
      },
      "source": [
        "# Function for showing a dataset of 2 classes\n",
        "\n",
        "def plot_dataset(X, y, axes):\n",
        "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
        "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
        "    plt.axis(axes)\n",
        "    plt.grid(True, which='both')\n",
        "    plt.xlabel(r\"$x_1$\", fontsize=20)\n",
        "    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcP4TIe_WSsO"
      },
      "source": [
        "# Plot function for showing decision boundary\n",
        "\n",
        "def plot_predictions(clf, axes):\n",
        "    x0s = np.linspace(axes[0], axes[1], 100)\n",
        "    x1s = np.linspace(axes[2], axes[3], 100)\n",
        "    x0, x1 = np.meshgrid(x0s, x1s)\n",
        "    X = np.c_[x0.ravel(), x1.ravel()]\n",
        "    y_pred = clf.predict(X).reshape(x0.shape)\n",
        "    y_decision = clf.decision_function(X).reshape(x0.shape)\n",
        "    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n",
        "    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7hWHbBhWSsP"
      },
      "source": [
        "### Generate a 2-class moon shaped dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fct-JOkWSsQ"
      },
      "source": [
        "from sklearn.datasets import make_moons\n",
        "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
        "\n",
        "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq4e-tqkWSsQ"
      },
      "source": [
        "### Using kernel SVM for general data (linear & nonlinear)\n",
        "Alternatively, often preferred, the moon data can be classified using SVC directly.\n",
        "\n",
        "Here we practice both Pipeline and make_pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8KRcYUgWSsR"
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "poly_kernel_svm_clf = make_pipeline(StandardScaler(), \n",
        "                                    SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
        "\n",
        "poly_kernel_svm_clf.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yYLHlu_WSsR"
      },
      "source": [
        "poly100_kernel_svm_clf = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm_clf\", SVC(kernel=\"poly\", degree=10, coef0=100, C=5))\n",
        "    ])\n",
        "poly100_kernel_svm_clf.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqeaucMjWSsS"
      },
      "source": [
        "fig, axes = plt.subplots(ncols=2, figsize=(10.5, 4), sharey=True)\n",
        "\n",
        "plt.sca(axes[0])\n",
        "plot_predictions(poly_kernel_svm_clf, [-1.5, 2.45, -1, 1.5])\n",
        "plot_dataset(X, y, [-1.5, 2.4, -1, 1.5])\n",
        "plt.title(r\"$d=3, r=1, C=5$\", fontsize=18)\n",
        "\n",
        "plt.sca(axes[1])\n",
        "plot_predictions(poly100_kernel_svm_clf, [-1.5, 2.45, -1, 1.5])\n",
        "plot_dataset(X, y, [-1.5, 2.4, -1, 1.5])\n",
        "plt.title(r\"$d=10, r=100, C=5$\", fontsize=18)\n",
        "plt.ylabel(\"\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqFU1l02WSsS"
      },
      "source": [
        "### Effects of hyperparameters\n",
        "Illustrate effects of hyperparameters. Here we only consider gamma of RBF and C. Note smaller gamma values specify wider bell shape, and smaller C values specify stronger regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYEQm8VyWSsT"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "gamma1, gamma2 = 0.1, 5\n",
        "C1, C2 = 0.001, 1000\n",
        "hyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2)\n",
        "\n",
        "svm_clfs = []\n",
        "for gamma, C in hyperparams:\n",
        "    rbf_kernel_svm_clf = Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"svm_clf\", SVC(kernel=\"rbf\", gamma=gamma, C=C))\n",
        "        ])\n",
        "    rbf_kernel_svm_clf.fit(X, y)\n",
        "    svm_clfs.append(rbf_kernel_svm_clf)\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10.5, 7), sharex=True, sharey=True)\n",
        "\n",
        "for i, svm_clf in enumerate(svm_clfs):\n",
        "    plt.sca(axes[i // 2, i % 2])\n",
        "    plot_predictions(svm_clf, [-1.5, 2.45, -1, 1.5])\n",
        "    plot_dataset(X, y, [-1.5, 2.45, -1, 1.5])\n",
        "    gamma, C = hyperparams[i]\n",
        "    plt.title(r\"$\\gamma = {}, C = {}$\".format(gamma, C), fontsize=16)\n",
        "    if i in (0, 1):\n",
        "        plt.xlabel(\"\")\n",
        "    if i in (1, 3):\n",
        "        plt.ylabel(\"\")    \n",
        "   \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1RUcqkkWSsT"
      },
      "source": [
        "### Hyperparameter optimisation using grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdwgzXWpWSsU"
      },
      "source": [
        "#from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# define search space\n",
        "parameters = {'kernel':('linear', 'rbf'), 'gamma':[0.1, 5], 'C':[0.001, 1, 10, 1000]}   \n",
        "\n",
        "# define classification model\n",
        "model = SVC(kernel=\"rbf\")\n",
        "\n",
        "# define evaluation\n",
        "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
        "\n",
        "# define search\n",
        "search = GridSearchCV(model, parameters, scoring='accuracy', n_jobs=-1, cv=cv)\n",
        "#search = GridSearchCV(model, parameters, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# execute search\n",
        "result = search.fit(X, y)\n",
        "\n",
        "# summarize result\n",
        "print('Best Score: %s' % result.best_score_)\n",
        "print('Best Hyperparameters: %s' % result.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9vCJoIOWSsU"
      },
      "source": [
        "### Exercise\n",
        "Optimise hyperparameters of the above model using randomised search: RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bMuczKVWSsV"
      },
      "source": [
        "## SVM for regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lovG0leBWSsV"
      },
      "source": [
        "# Define a plot function for regression data\n",
        "def plot_svm_regression(svm_reg, X, y, axes):\n",
        "    x1s = np.linspace(axes[0], axes[1], 100).reshape(100, 1)\n",
        "    y_pred = svm_reg.predict(x1s)\n",
        "    plt.plot(x1s, y_pred, \"k-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
        "    plt.plot(x1s, y_pred + svm_reg.epsilon, \"k--\")\n",
        "    plt.plot(x1s, y_pred - svm_reg.epsilon, \"k--\")\n",
        "    plt.scatter(X[svm_reg.support_], y[svm_reg.support_], s=180, facecolors='#FFAAAA')\n",
        "    plt.plot(X, y, \"bo\")\n",
        "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
        "    plt.legend(loc=\"upper left\", fontsize=18)\n",
        "    plt.axis(axes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO_G4d3jWSsW"
      },
      "source": [
        "### Generate data for regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nle25qrWSsW"
      },
      "source": [
        "np.random.seed(42)\n",
        "m = 100\n",
        "X = 2 * np.random.rand(m, 1) - 1\n",
        "y = (0.2 + 0.1 * X + 0.5 * X**2 + np.random.randn(m, 1)/10).ravel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9YTS0X2WSsX"
      },
      "source": [
        "### Fit two SVR models using different regularisation C"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWuYWLuEWSsX"
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "svm_poly_reg1 = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1, gamma=\"scale\")\n",
        "svm_poly_reg2 = SVR(kernel=\"poly\", degree=2, C=0.01, epsilon=0.1, gamma=\"scale\")\n",
        "svm_poly_reg1.fit(X, y)\n",
        "svm_poly_reg2.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OzjLlYhWSsY"
      },
      "source": [
        "### Visualse models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G7HIpl7WSsZ"
      },
      "source": [
        "fig, axes = plt.subplots(ncols=2, figsize=(9, 4), sharey=True)\n",
        "plt.sca(axes[0])\n",
        "plot_svm_regression(svm_poly_reg1, X, y, [-1, 1, 0, 1])\n",
        "plt.title(r\"$degree={}, C={}, \\epsilon = {}$\".format(svm_poly_reg1.degree, svm_poly_reg1.C, svm_poly_reg1.epsilon), fontsize=18)\n",
        "plt.ylabel(r\"$y$\", fontsize=18, rotation=0)\n",
        "plt.sca(axes[1])\n",
        "plot_svm_regression(svm_poly_reg2, X, y, [-1, 1, 0, 1])\n",
        "plt.title(r\"$degree={}, C={}, \\epsilon = {}$\".format(svm_poly_reg2.degree, svm_poly_reg2.C, svm_poly_reg2.epsilon), fontsize=18)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E0rHM9DWSsZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}