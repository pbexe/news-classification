{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FeatureEngineeringSelection_Sklearn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.6 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "e9edc7c343a24c288550b17b6dde5d80d15c0458da4e16379f6e1a66c9f3762f"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k3zTKjPSXjm"
      },
      "source": [
        "*-- Author: Jose Camacho Collados (Cardiff University, Lecturer) --*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG7F15TeVuxb"
      },
      "source": [
        "This Jupyter notebook is a first approximation to Machine Learning using the library scikit-learn. In this notebook we will train a machine learning algorithm for the first time! Feature engineering and feature selection are also included below.\n",
        "\n",
        "Let's start with a simple introduction to scikit-learn, which is the Machine Learning library we are going to use during the first part of the module.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXTVxIXysM9O"
      },
      "source": [
        "## INTRODUCTION TO SCIKIT-LEARN\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "First, we import the libraries that we are going to use: numpy (vector manipulation), nltk (text preprocessing) and scikit-learn (machine learning).\n",
        "\n",
        "**Note:** All these libraries need to be downloaded beforehand if not using Google Colab. Check their official websites for details on how to install them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Nv6UldrVuXv"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import sklearn\n",
        "import operator\n",
        "import requests\n",
        "nltk.download('stopwords') # If needed\n",
        "nltk.download('punkt') # If needed\n",
        "nltk.download('wordnet') # If needed"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\conta\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\conta\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\conta\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLX-9_KS2Xer"
      },
      "source": [
        "We are going to work with a binary classification dataset, named \"Diabetes\", with the goal of predicting whether a person has diabetes or not. First, we need to load the dataset in Python. There are three different ways to download the dataset:\n",
        "\n",
        "\n",
        "1.   (General) Load directly from the web. \n",
        "2.   (Google Colab) Download manually the dataset from the web or Learning Central, add it to your Google Drive and load it from there.\n",
        "3.   (Local) Download manually the dataset from the web or Learning Central, and load it directly from your hard drive.\n",
        "\n",
        "Choose your favorite method below, and un/comment out the two other methods that you will not be using (remember you can comment lines of code by adding *#* at the beginning).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR3p8oif2EC6"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "\n",
        "#Method 1\n",
        "response = requests.get(url)\n",
        "dataset_file = response.text.split(\"\\n\")\n",
        "\n",
        "#Method 2 - Google Colab\n",
        "##from google.colab import drive\n",
        "##drive.mount('/content/drive/')\n",
        "##path= '/content/drive/My Drive/pima-indians-diabetes.data.csv'\n",
        "##dataset_file=open(path).readlines()\n",
        "\n",
        "\n",
        "#Method 3 - Local\n",
        "##path='/home/user/Downloads/pima-indians-diabetes.data.csv'\n",
        "##dataset_file=open(path).readlines()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFZD0PHFR9Xd"
      },
      "source": [
        "The dataset is stored as a .csv (comma-separated) file. In the following we are going to access the data, each file corresponding to a patient and their diagnostic measures (we will refer to these as features). In total, there are eight features, sorted from left to right:\n",
        "\n",
        "1.   Number of times pregnant.\n",
        "2.   Plasma glucose concentration a 2 hours in an oral glucose tolerance test.\n",
        "3.   Diastolic blood pressure (mm Hg).\n",
        "4.   Triceps skinfold thickness (mm).\n",
        "5.   2-Hour serum insulin (mu U/ml).\n",
        "6.   Body mass index (weight in kg/(height in m)^2).\n",
        "7.   Diabetes pedigree function.\n",
        "8.   Age (years).\n",
        "\n",
        "The last column corresponds to whether the patient has diabetes (1) or not (0). This is the feature we want to predict. \n",
        "\n",
        "Let's check how the data looks like by, for example, checking the number of patients overall and the features of the first five patients of the dataset:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0d44137XBKD"
      },
      "source": [
        "**Note:** If you use `.readlines()` (as in Methods 2 and 3), note that your lines may contain line breaks. To remove line breaks and leading/trailing whitespaces (if there were) you can use the `.strip()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1xFulUdI10f"
      },
      "source": [
        "print (\"Number of patients: \"+str(len(dataset_file))+\"\\n\")\n",
        "for patient_line in dataset_file[:5]:\n",
        "  print (patient_line) # or print (patient_line.strip())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of patients: 768\n\n6,148,72,35,0,33.6,0.627,50,1\n1,85,66,29,0,26.6,0.351,31,0\n8,183,64,0,0,23.3,0.672,32,1\n1,89,66,23,94,28.1,0.167,21,0\n0,137,40,35,168,43.1,2.288,33,1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SZrtiMeJcTf"
      },
      "source": [
        "As we can observed in this small sample of five patients, the first, third and fifth got diabetes (last column=1), while the second and fourth did not (last column=0).\n",
        " \n",
        "**Note:** This dataset contains a few missing values, which are set as zeroes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzU_2VPuNhpn"
      },
      "source": [
        "**Excercise (optional):**\n",
        "Try to load and process the csv file using [pandas](https://pandas.pydata.org). Pandas is a library to process data structures (e.g. csv files) and also provides useful data analysis tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4XjUJfoI2Y_"
      },
      "source": [
        "To train our machine learning classifier, we first need to convert the input features of each person into vectors (numpy arrays) and keep that information into a list. Similary, we keep the output for each person (1 or 0 depending whether the person has diabetes or not) in another list.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sKz-jCdvOuU"
      },
      "source": [
        "X_train=[]\n",
        "Y_train=[]\n",
        "for patient_line in dataset_file:\n",
        "  patient_linesplit=patient_line.split(\",\")\n",
        "  vector_patient_features=np.zeros(len(patient_linesplit)-1)\n",
        "  for i in range(len(patient_linesplit)-1):\n",
        "    vector_patient_features[i]=float(patient_linesplit[i])\n",
        "  X_train.append(vector_patient_features)\n",
        "  Y_train.append(int(patient_linesplit[-1]))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guExdzmCWpu7"
      },
      "source": [
        "Once we preprocessed the data, we are ready to train our first machine learning algorithm! In this case we are going to use an SVM binary classifier (we will see more details about machine learning algorithms from Session 4). As a binary classifier, for training we should provide the features as input and \"1\" or \"0 as output. The function to train a machine learning model in sklearn is `.fit`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJlXph8NWqsn"
      },
      "source": [
        "X_train_diabetes=np.asarray(X_train)\n",
        "Y_train_diabetes=np.asarray(Y_train) # This step is really not necessary, but it is recommended to work with numpy arrays instead of Python lists.\n",
        "\n",
        "svm_clf_diabetes=sklearn.svm.SVC(kernel=\"linear\",gamma='auto') # Initialize the SVM model\n",
        "svm_clf_diabetes.fit(X_train_diabetes,Y_train_diabetes) # Train the SVM model"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(gamma='auto', kernel='linear')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJOfaaWENQwj"
      },
      "source": [
        "We have already trained our first supervised machine learning classifier! Let's check now how it works with two random patients:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wprWeeVANZZm"
      },
      "source": [
        "patient_1=['0', '100', '86', '20', '39', '35.1', '0.242', '21']\n",
        "patient_2=['1', '197', '70', '45', '543', '30.5', '0.158', '51']\n",
        "print (svm_clf_diabetes.predict([patient_1]))\n",
        "print (svm_clf_diabetes.predict([patient_2]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\n[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAke3d5QIAJS"
      },
      "source": [
        "**Excercise 1:**\n",
        "Choose three features from the eight features of the \"Diabetes\" dataset and learn the same binary SVM classifier. Check how the classifier works with one example, i.e., choose random values for your three features and check the prediction of your SVM classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIAxWjDxZhuV"
      },
      "source": [
        "X_train=[]\n",
        "Y_train=[]\n",
        "#To complete"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7m0tAb_-aTq"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "## Feature engineering\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In Machine Learning, the process of feature engineering consists of transforming data into features. In the previous examples with the \"Diabetes\" dataset the features were already given, but in most cases we should extract the features ourselves. In this case, we are going to deal with examples with textual data. To extract features from textual content, we can make use of what we learned from the exercises of Session 1.\n",
        "\n",
        "For these exercises we will be using a dataset for *sentiment analysis*. Sentiment analysis is the automatic process of classifying opinions as positive or negative (there are other definitions of sentiment analysis which are more general as well). To do so, we are going to make use of the RT-polarity dataset. Let's first download it and inspect the data. This time we are going to load the dataset directly from the web, but feel free to use the method of your choice to load the data, as explained above: \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRlEgUcvIw_f"
      },
      "source": [
        "url_pos=\"http://josecamachocollados.com/rt-polarity.pos.txt\" # Containing all positive reviews, one review per line\n",
        "url_neg=\"http://josecamachocollados.com/rt-polarity.neg.txt\" # Containing all negative reviews, one review per line\n",
        "#Load positive reviews\n",
        "response_pos = requests.get(url_pos)\n",
        "dataset_file_pos = response_pos.text.split(\"\\n\")\n",
        "\n",
        "#Load negative reviews\n",
        "response_neg = requests.get(url_neg)\n",
        "dataset_file_neg = response_neg.text.split(\"\\n\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vln_3gb4zV_y"
      },
      "source": [
        "Let's inspect a bit the dataset, by printing the first five positive and negative reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAFJ2MKszXQK"
      },
      "source": [
        "print (\"Positive reviews:\\n\")\n",
        "for pos_review in dataset_file_pos[:5]:\n",
        "  print (pos_review)\n",
        "print (\"\\n   ------\\n\")  \n",
        "print (\"Negative reviews:\\n\")\n",
        "for neg_review in dataset_file_neg[:5]:\n",
        "  print (neg_review)\n",
        " "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive reviews:\n\nthe rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \nthe gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \neffective but too-tepid biopic\nif you sometimes like to go to the movies to have fun , wasabi is a good place to start . \nemerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \n\n   ------\n\nNegative reviews:\n\nsimplistic , silly and tedious . \nit's so laddish and juvenile , only teenage boys could possibly find it funny . \nexploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \na visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rggiZRURumOu"
      },
      "source": [
        "Now we are going to try to define a vocabulary which can be used to transform sentences (strings) into text. Let's take, for example, the 1000 most frequent words in the dataset, excluding stopwords.\n",
        "\n",
        "**Note:** Stopwords are generally short function words that do not provide a specific meaning without context (e.g. articles such as \"the\" or prepositions such as \"on\"). They can be different depending on the purpose. In our case we will be using the English stopwords as given by NLTK.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaLXuRezxrwJ"
      },
      "source": [
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "# Function taken from Session 1\n",
        "def get_list_tokens(string):\n",
        "  sentence_split=nltk.tokenize.sent_tokenize(string)\n",
        "  list_tokens=[]\n",
        "  for sentence in sentence_split:\n",
        "    list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n",
        "    for token in list_tokens_sentence:\n",
        "      list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
        "  return list_tokens"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGgaqwNuUwLd"
      },
      "source": [
        "# First, we get the stopwords list from nltk\n",
        "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
        "# We can add more words to the stopword list, like punctuation marks\n",
        "stopwords.add(\".\")\n",
        "stopwords.add(\",\")\n",
        "stopwords.add(\"--\")\n",
        "stopwords.add(\"``\")\n",
        "\n",
        "# Now we create a frequency dictionary with all words in the dataset\n",
        "# This can take a few minutes depending on your computer, since we are processing more than ten thousand sentences\n",
        "\n",
        "dict_word_frequency={}\n",
        "for pos_review in dataset_file_pos:\n",
        "  sentence_tokens=get_list_tokens(pos_review)\n",
        "  for word in sentence_tokens:\n",
        "    if word in stopwords: continue\n",
        "    if word not in dict_word_frequency: dict_word_frequency[word]=1\n",
        "    else: dict_word_frequency[word]+=1\n",
        "for neg_review in dataset_file_neg:\n",
        "  sentence_tokens=get_list_tokens(neg_review)\n",
        "  for word in sentence_tokens:\n",
        "    if word in stopwords: continue\n",
        "    if word not in dict_word_frequency: dict_word_frequency[word]=1\n",
        "    else: dict_word_frequency[word]+=1\n",
        "      \n",
        "# Now we create a sorted frequency list with the top 1000 words, using the function \"sorted\". Let's see the 15 most frequent words\n",
        "sorted_list = sorted(dict_word_frequency.items(), key=operator.itemgetter(1), reverse=True)[:1000]\n",
        "i=0\n",
        "for word,frequency in sorted_list[:15]:\n",
        "  i+=1\n",
        "  print (str(i)+\". \"+word+\" - \"+str(frequency))\n",
        "  \n",
        "# Finally, we create our vocabulary based on the sorted frequency list \n",
        "vocabulary=[]\n",
        "for word,frequency in sorted_list:\n",
        "  vocabulary.append(word)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. 's - 3537\n2. film - 1770\n3. movie - 1547\n4. n't - 940\n5. one - 763\n6. ha - 732\n7. like - 722\n8. ' - 552\n9. story - 536\n10. make - 530\n11. character - 483\n12. time - 442\n13. doe - 417\n14. comedy - 389\n15. much - 386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jB5b2IS8ZzK"
      },
      "source": [
        "Once we have our vocabulary, we can transform sentences into vectors as we saw in Session 1, using the function below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZrrwpmUud2Y"
      },
      "source": [
        "def get_vector_text(list_vocab,string):\n",
        "  vector_text=np.zeros(len(list_vocab))\n",
        "  list_tokens_string=get_list_tokens(string)\n",
        "  for i, word in enumerate(list_vocab):\n",
        "    if word in list_tokens_string:\n",
        "      vector_text[i]=list_tokens_string.count(word)\n",
        "  return vector_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euSGxH85-r5z"
      },
      "source": [
        "Using this function we can now load our training features, as we did with the \"Diabetes\" dataset. In this case, we will label positive reviews as \"1\" and negative reviews as \"0\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNMPj692-uU0"
      },
      "source": [
        "# This can take a while, as we are converting more than ten thousand sentences into vectors!\n",
        "X_train=[]\n",
        "Y_train=[]\n",
        "for pos_review in dataset_file_pos:\n",
        "  vector_pos_review=get_vector_text(vocabulary,pos_review)\n",
        "  X_train.append(vector_pos_review)\n",
        "  Y_train.append(1)\n",
        "for neg_review in dataset_file_neg:\n",
        "  vector_neg_review=get_vector_text(vocabulary,neg_review)\n",
        "  X_train.append(vector_neg_review)\n",
        "  Y_train.append(0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge5heo2N-9m8"
      },
      "source": [
        "**Exercise (optional):** Try transforming the sentences into weighted frequency features using [TFidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). This function uses a weighted scheme called [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (term frequency-inverse document frequency) which basically penalizes words that are repeated across many documents (e.g. frequent words such as \"the\" or \"a\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYlpPJlz-NxK"
      },
      "source": [
        "Once we have loaded all the feature vectors, we can now train our SVM binary classifier! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eRzTNWv8h0t"
      },
      "source": [
        "X_train_sentanalysis=np.asarray(X_train)\n",
        "Y_train_sentanalysis=np.asarray(Y_train)\n",
        "\n",
        "svm_clf_sentanalysis=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
        "svm_clf_sentanalysis.fit(X_train_sentanalysis,Y_train_sentanalysis) # Train the SVM model. This may also take a while.\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(gamma='auto', kernel='linear')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6ReLfgDyNbQ"
      },
      "source": [
        "Let's try how it works with some examples!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH4VcPPXAhdM"
      },
      "source": [
        "sentence_1=\"Fascinating, I loved it.\"\n",
        "sentence_2=\"Bad movie, probably one of the worst I have ever seen.\"\n",
        "print (svm_clf_sentanalysis.predict([get_vector_text(vocabulary,sentence_1)]))\n",
        "print (svm_clf_sentanalysis.predict([get_vector_text(vocabulary,sentence_2)]))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdrfQRRuAh28"
      },
      "source": [
        "It seems to be working! However, this is a very simple classifier and is definetely not perfect. You can try other examples yourself to see how the model behaves, find weaknesses and try to improve it with better features!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lP9GC-tEDcM"
      },
      "source": [
        "**Excercise 2:**\n",
        "Based on this example, create a function that, given two files of positive and negative reviews (one sentence per line as in our RT-polarity dataset) and an integer number X as input, it returns the vocabulary and a binary SVM classifier similar to what we learned, using the X most frequent words as features. Check how the classifier works with X=1200. You can check the predictions with the same sample sentences as above.\n",
        "\n",
        "**Note:** You can use auxiliary functions if needed (not mandatory but can be useful). For example, a function that first retrieves the vocabulary given the datasets and X."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJcDFw7iFBvR"
      },
      "source": [
        "def train_svm_classifier(dataset_file_pos, dataset_file_neg, x):\n",
        "  #To complete"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unexpected EOF while parsing (<ipython-input-27-84a5e854f0c6>, line 2)",
          "traceback": [
            "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-27-84a5e854f0c6>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    #To complete\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObiYSdEYvk_t"
      },
      "source": [
        "**Exercise (optional):** Think about different features that can be useful for sentiment analysis and add it to our frequency vector. Some ideas: (1) use a dictionary of positive or negative words (some dictionaries available [here](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon)); (2) use n-gram features (n-grams are sequence of n-words as opposed or a single word, e.g., \"cardiff university\" would be a bigram); (3) use only verbs and adjectives as features (see [PoS tagging](https://www.nltk.org/book/ch05.html) in NLTK)..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OBMzQ8cB8C7"
      },
      "source": [
        "\n",
        "## Feature selection\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The process of feature selection consists of selecting a subset of relevant features. For example, in the sentiment analysis example above, we selected 1000 features based on the 1000 most frequent words. However, not all words may be equally relevant. For example, \"film\" is the second most frequent word but may appear equally in positive and negative reviews, therefore it is not a very relevant feature for our task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRSPZN1CGD_o"
      },
      "source": [
        "In this notebook we are going to use the [chi-squared test](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection) method, available in sklearn. This method basically removes the features that appear to be irrelevant to a given class (in our case positive or negative). For example, words that do not express sentiment are expected to be removed from the set. Let's apply this feature selection method to our RT-polarity dataset to keep only the 500 most relevant features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-0HOolhHnxV"
      },
      "source": [
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_selection import SelectKBest"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wKZ2b4vgquE"
      },
      "source": [
        "fs_sentanalysis=SelectKBest(chi2, k=500).fit(X_train_sentanalysis, Y_train_sentanalysis)\n",
        "X_train_sentanalysis_new = fs_sentanalysis.transform(X_train_sentanalysis)\n",
        "#X_train_new = SelectKBest(chi2, k=500).fit_transform(X_train, Y_train)\n",
        "print (\"Size original training matrix: \"+str(X_train_sentanalysis.shape))\n",
        "print (\"Size new training matrix: \"+str(X_train_sentanalysis_new.shape))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size original training matrix: (10664, 1000)\nSize new training matrix: (10664, 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp34c7NTJxP2"
      },
      "source": [
        "Now we can train again our SVM classifier with the 500 most relevant features, replacing the old one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDl987FvH6IX"
      },
      "source": [
        "svm_clf_sentanalysis_=sklearn.svm.SVC(kernel=\"linear\",gamma='auto') # Change the name here, e.g. 'new sentanalysis_svm_clf', and below if you don't want to replace your old classifier.\n",
        "svm_clf_sentanalysis_.fit(X_train_sentanalysis_new,Y_train_sentanalysis) #Train the new SVM model. This may take a while."
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(gamma='auto', kernel='linear')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_n4Q6DtKP78"
      },
      "source": [
        "And now we can test our classifier with some new examples.\n",
        "\n",
        "**Note**: To transform the original 1000 features into our reduced 500 features, we use the function `.transform`. This function is very common in sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdAXTa30IYd4"
      },
      "source": [
        "sentence_3=\"Highly recommended: it was a fascinating film.\"\n",
        "sentence_4=\"I got a bit bored, it was not what I was expecting.\"\n",
        "print (svm_clf_sentanalysis_.predict(fs_sentanalysis.transform([get_vector_text(vocabulary,sentence_3)])))\n",
        "print (svm_clf_sentanalysis_.predict(fs_sentanalysis.transform([get_vector_text(vocabulary,sentence_4)])))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MejtAgMhON64"
      },
      "source": [
        "**Exercise 3:** Apply the same chi-squared feature selection method to select the seven most relevant features from the Diabetes dataset. Check your method with some sample input features (you can use the same \"patient_1\" and \"patient_2\" examples)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlHAEcTNP9eV"
      },
      "source": [
        "# To complete"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awsktnSuKZ_W"
      },
      "source": [
        "**Exercise (optional):** Check other feature selection methods in skelarn (feature selection methods available [here](https://scikit-learn.org/stable/modules/feature_selection.html)) and try one of them with our sentiment analysis dataset."
      ]
    }
  ]
}