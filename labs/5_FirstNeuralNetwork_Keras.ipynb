{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_FirstNeuralNetwork_Keras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG7F15TeVuxb"
      },
      "source": [
        "This Jupyter notebook includes a simple design of a neural network using the [Keras](https://keras.io/) library. Keras is a high-level neural network library running on top of machine learning platforms such as TensorFlow.\n",
        "\n",
        "This notebook also includes some tips and common practical functions when training a neural network.  At the end of this exercise you will have already implemented your first neural network in Python!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAnvJOAnTYfa"
      },
      "source": [
        "This notebook is based on the following [blog post](https://towardsdatascience.com/building-our-first-neural-network-in-keras-bdc8abbc17f5) (you can check it for more details)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXTVxIXysM9O"
      },
      "source": [
        "## TRAINING A NEURAL NETWORK\n",
        "\n",
        "---\n",
        "\n",
        "As usual, we first import the libraries that we are going to use, including now Keras.\n",
        "\n",
        "**Note:** All these libraries need to be downloaded beforehand if not using Google Colab. Check their official websites for details on how to install them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Nv6UldrVuXv"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import sklearn\n",
        "import keras\n",
        "import requests\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLX-9_KS2Xer"
      },
      "source": [
        "We are going to use a dataset that contains mobile phone features. Our goal is to build a neural network that can predict the price range of a mobile phone given its features (e.g. batery power or whether it accepts dual sim or not). For this datasets four price ranges are used:\n",
        "\n",
        "*   0 - low cost\n",
        "*   1 - medium cost\n",
        "*   2 - high cost\n",
        "*   3 - very high cost\n",
        "\n",
        "More information about the dataset can be found [here](https://www.kaggle.com/iabhishekofficial/mobile-price-classification).\n",
        "\n",
        "We are going to load the dataset directly from the internet, but remember from the first semester that you can also load your dataset locally if you wish, or through Google Drive if using Google Colab:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR3p8oif2EC6"
      },
      "source": [
        "url_train=\"http://josecamachocollados.com/mobilephone_train.csv\" # Containing the training set of the mobile phone dataset, one phone per line\n",
        "url_test=\"http://josecamachocollados.com/mobilephone_test.csv\" # Containing the test set of the mobile phone dataset, one phone per line\n",
        "\n",
        "#Load training set\n",
        "response_train = requests.get(url_train)\n",
        "dataset_file_train = response_train.text.split(\"\\n\")\n",
        "\n",
        "#Load test set\n",
        "response_test = requests.get(url_test)\n",
        "dataset_file_test = response_test.text.split(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIyQsNSvd5wr"
      },
      "source": [
        "The first line of each set contains all different feature names. Let's now print the features and the first five instances of the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGig7uiMd-DV"
      },
      "source": [
        "print (dataset_file_train[0])\n",
        "print (\"  ----  \\n\")\n",
        "for line in dataset_file_train[1:6]:\n",
        "  print (line) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwJaJMhqPkQs"
      },
      "source": [
        "As usual, we are going to load the features in a vector form. In this case we are going to create a numpy matrix with the features and another one with the labels (i.e. the price in this case) for the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq8h2O22RkWs"
      },
      "source": [
        "dataset_train_noheader=dataset_file_train[1:] # We remove the header with the name of the features\n",
        "random.shuffle(dataset_train_noheader) # We shuffle the instances of the dataset\n",
        "\n",
        "len_dataset_train=len(dataset_train_noheader) # Number of instances in the training set\n",
        "num_features_train=len(dataset_train_noheader[1].split(\",\"))-1 # Number of features in the dataset\n",
        "print (\"The training set has \"+str(len_dataset_train)+\" instances and \"+str(num_features_train)+\" features per instance\")\n",
        "\n",
        "X_train=np.zeros((len_dataset_train,num_features_train)) # Initialization of the feature array of the training set\n",
        "Y_train=np.zeros((len_dataset_train,1)) # Initialization of the label array of the training set\n",
        "for i,line in enumerate(dataset_train_noheader):\n",
        "  linesplit=line.strip().split(\",\")\n",
        "  for j,feature in enumerate(linesplit[:-1]):\n",
        "    X_train[i,j]=float(feature)\n",
        "  Y_train[i,0]=int(linesplit[-1])\n",
        "print (\"Training set has now been loaded into a numpy matrix\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W34EfgiARiVh"
      },
      "source": [
        "**Note:** Remember that you can also load a csv file using [pandas](https://pandas.pydata.org) if you prefer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CGwlOLAZEcB"
      },
      "source": [
        "**Excercise (Optional):** Repeat the same process to build the numpy training set by using pandas (`X_train` and `Y_train` should have the same values at the end)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n7mLsGBPkmU"
      },
      "source": [
        "One issue with most neural networks (and machine learning learning in general) is that they may not cope well with features with different ranges. To this end, it is often advisable to normalize all features to a common scale. We can do that using the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) of sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMbKZUkYWd4k"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdiRT0gwWld3"
      },
      "source": [
        "sc = StandardScaler()\n",
        "X_train_norm = sc.fit_transform(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZLeOfaqaaUs"
      },
      "source": [
        "Now we need to convert the labels into one-hot vectors. This is done to be able to integrate all possible predictions into the same vector. In our case, where we have four labels, each label will get converted to the following one-hot vector:\n",
        "\n",
        "*   0 - (1,0,0,0)\n",
        "*   1 - (0,1,0,0)\n",
        "*   2 - (0,0,1,0)\n",
        "*   3 - (0,0,0,1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9GEvq0mXkpb"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOYYeGPoXmHd"
      },
      "source": [
        "ohe = OneHotEncoder()\n",
        "Y_train_onehot = ohe.fit_transform(Y_train).toarray()\n",
        "size_output_onehot=len(Y_train_onehot[0])\n",
        "print (\"Size of the one-hot label vectors: \"+str(size_output_onehot))\n",
        "print (Y_train_onehot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7OpSVwcaai1"
      },
      "source": [
        "Now it's time to build our neural network! With Keras it is quite simple, but you still need to define the structure and size of your network. For example, let's build a neural network of one hidden layer with 15 dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQYwj4TkYVzn"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKMHo7oSYYDY"
      },
      "source": [
        "model = Sequential() # This will be our neural network. Sequential means that the output of each layer is the input for the next one.\n",
        "model.add(Dense(15, input_dim=num_features_train, activation='relu')) # Our network has as input the same number of features of the training set (i.e. 20 in our case).\n",
        "model.add(Dense(size_output_onehot, activation='softmax')) # The output layer has in this case the same number of dimensions as our label one-hot vectors (i.e. 4 in our case).\n",
        "print (model.summary()) # .summary shows an overview of our model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # This serves to define the loss function and optimizer."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qyFDT0aaUYZ"
      },
      "source": [
        "Let's now train our model! The process is similar as in sklearn, as we simply need to use the `.fit` function. Let's use 250 epochs for example (the number of epochs is the number of times our model will iterate through the entire training set)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdsDDtiycQxs"
      },
      "source": [
        "**Note:** If the cell for training is run more than once, the model will start training from where it finished to train the previous time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q6j9xvWaz4b"
      },
      "source": [
        "history = model.fit(X_train_norm, Y_train_onehot, epochs=250, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4X-K4WTAcFb"
      },
      "source": [
        "We can visually see how our model is training and its accuracy per epoch with the Python library [matplotlib](https://matplotlib.org/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXpMbxdyAd2y"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K28QtBYGAe2D"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DX-xeSZaaqQ"
      },
      "source": [
        "Let's now check the performance of our trained model on the test data. For that, we need first to preprocess the test set as we did with the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo50ui-7dFdA"
      },
      "source": [
        "dataset_test_noheader=dataset_file_test[1:]\n",
        "random.shuffle(dataset_test_noheader)\n",
        "\n",
        "len_dataset_test=len(dataset_test_noheader) # Number of instances in the test set\n",
        "num_features_test=len(dataset_test_noheader[1].split(\",\"))-1 # Number of features in the test set (should be the same as the training set)\n",
        "print (\"The test set has \"+str(len_dataset_test)+\" instances and \"+str(num_features_test)+\" features per instance\")\n",
        "\n",
        "X_test=np.zeros((len_dataset_test,num_features_test)) # Initialization of the feature array of the test set\n",
        "Y_test=np.zeros((len_dataset_test,1)) # Initialization of the label array of the test set\n",
        "for i,line in enumerate(dataset_test_noheader):\n",
        "  linesplit=line.strip().split(\",\")\n",
        "  for j,feature in enumerate(linesplit[:-1]):\n",
        "    X_test[i,j]=float(feature)\n",
        "  Y_test[i,0]=int(linesplit[-1])\n",
        "print (\"Test set has now been loaded into a numpy matrix\")\n",
        "X_test_norm = sc.transform(X_test) # Finally, we normalized the features of the test set using our previously defined sc function.\n",
        "Y_test_onehot = ohe.transform(Y_test).toarray() # We also transform the labels in the test set into the same one-hot vectors."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CR273IQaatM"
      },
      "source": [
        "**Excercise (optional):** Define a function for: given a dataset with the same format as the mobine phone dataset as input, return the `X_train` and `Y_train` arrays used to trained our model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntOEosJPaaxF"
      },
      "source": [
        "Then, once having the test set preprocessed, we can finally test our model. For this we can use the `.predict` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks38LSdheuH-"
      },
      "source": [
        "Y_pred = model.predict(X_test_norm)\n",
        "preds = list()\n",
        "for i in range(len(Y_pred)):\n",
        "    preds.append(np.argmax(Y_pred[i])) #This serves to convert the hot encoded test label vectors to labels\n",
        "Y_test_gold = list()\n",
        "for i in range(len(Y_test_onehot)):\n",
        "    Y_test_gold.append(np.argmax(Y_test_onehot[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQN9_7gfA-sc"
      },
      "source": [
        "Let's now get the accuracy performance on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ2Y7LxxBBgW"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2iOXa6_BCi6"
      },
      "source": [
        "accuracy_test = accuracy_score(preds,Y_test_gold)\n",
        "print('The accuracy in the test set is: '+str(round(accuracy_test,3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGhLHgI4gCPW"
      },
      "source": [
        "**Excercise 1:** Build another neural network using Keras, in this case with two hidden layers of 15 and 10 dimensions each and trained on 100 epochs. Train your model and get the output result in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usK2GEuhdyLf"
      },
      "source": [
        "model_2 = Sequential() # To complete...\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nco3Q5yPYskA"
      },
      "source": [
        "**Exercise (optional):** Implement the same model but using a development set (you can split the training set into training and development for example). Then, use this development set to check the best performance per epoch and get the result of the model resulting from training until the best performing epoch. More details on how to do that can be found in the reference [blog post](https://towardsdatascience.com/building-our-first-neural-network-in-keras-bdc8abbc17f5). "
      ]
    }
  ]
}