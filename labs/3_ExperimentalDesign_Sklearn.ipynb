{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_ExperimentalDesign_Sklearn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.6 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "2a5068d00c7e668f630311d69b177f3bf5e34ccc777378605e74da0c3f384e51"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k3zTKjPSXjm"
      },
      "source": [
        "*-- Author: Jose Camacho Collados (Cardiff University, Lecturer) --*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG7F15TeVuxb"
      },
      "source": [
        "This Jupyter notebook includes exercises for understanding experimental design in Machine Learning. In this notebook we will introduce common evaluation measures in supervised machine learning. You will also be able to split your dataset in train, development and test, and also understand how cross-validation works. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXTVxIXysM9O"
      },
      "source": [
        "## EXPERIMENTAL DESIGN\n",
        "\n",
        "---\n",
        "\n",
        "First, we import the libraries that we are going to use, including as usual numpy (vector manipulation), nltk (text preprocessing) and scikit-learn (machine learning).\n",
        "\n",
        "**Note:** All these libraries need to be downloaded beforehand if not using Google Colab. Check their official websites for details on how to install them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Nv6UldrVuXv"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import sklearn\n",
        "import operator\n",
        "import requests\n",
        "nltk.download('stopwords') # If needed\n",
        "nltk.download('punkt') # If needed\n",
        "nltk.download('wordnet') # If needed"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\conta\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\conta\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\conta\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLX-9_KS2Xer"
      },
      "source": [
        "## A) TRAIN, DEVELOPMENT AND TEST SPLITS\n",
        "\n",
        "To start with, we are going to work with the same sentiment analysis dataset used in the previous session, i.e., RT-polarity. First, as usual, we need to load the dataset in Python. We are going to load it directly from the internet, but remember from the previous session that you can also load your dataset locally if you wish, or through Google Colab:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR3p8oif2EC6"
      },
      "source": [
        "url_pos=\"http://josecamachocollados.com/rt-polarity.pos.txt\" # Containing all positive reviews, one review per line\n",
        "url_neg=\"http://josecamachocollados.com/rt-polarity.neg.txt\" # Containing all negative reviews, one review per line\n",
        "\n",
        "#Load positive reviews\n",
        "response_pos = requests.get(url_pos)\n",
        "dataset_file_pos = response_pos.text.split(\"\\n\")\n",
        "\n",
        "#Load negative reviews\n",
        "response_neg = requests.get(url_neg)\n",
        "dataset_file_neg = response_neg.text.split(\"\\n\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGhLHgI4gCPW"
      },
      "source": [
        "Now we are going to split the dataset into training and test splits. First, we need to put together positive and negative reviews into a single list. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq35io6Dj7cG"
      },
      "source": [
        "dataset_full=[]\n",
        "for pos_review in dataset_file_pos:\n",
        "  dataset_full.append((pos_review,1))\n",
        "for neg_review in dataset_file_neg:\n",
        "  dataset_full.append((neg_review,0))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEtWB_f3kRZ7"
      },
      "source": [
        "**Note:** Remember that positive reviews are going to be labelled as \"1\" and negative reviews as \"0\". To store reviews with their corresponding labels, we have used tuples of the form `(review,label)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA55oh0Ykc81"
      },
      "source": [
        "With the full dataset stored in a single list, we are going to split our dataset into training and test, by following a standard 80%/20% distribution. We are going to randomly extract examples from the original dataset, 80% for the training set, and 20% for the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jdN4LaXhm_O"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import random"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBlU4Zyehoif"
      },
      "source": [
        "size_dataset_full=len(dataset_full)\n",
        "size_test=int(round(size_dataset_full*0.2,0))\n",
        "\n",
        "list_test_indices=random.sample(range(size_dataset_full), size_test)\n",
        "train_set=[]\n",
        "test_set=[]\n",
        "for i,example in enumerate(dataset_full):\n",
        "  if i in list_test_indices: test_set.append(example)\n",
        "  else: train_set.append(example)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAke3d5QIAJS"
      },
      "source": [
        "**Excercise (Optional):**\n",
        "Use the function [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from sklearn to split  the original RT-polarity dataset into training and test. More information in this [blog post](https://medium.com/@contactsunny/how-to-split-your-dataset-to-train-and-test-datasets-using-scikit-learn-e7cf6eb5e0d).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzK6cEyej3LV"
      },
      "source": [
        "To double-check that we have split the dataset as we planned to, let's check the final sizes. We are going to also shuffle the examples in each of the splits (using the function `random.shuffle`), as it is recommended in many cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JvrD8ACpAbM"
      },
      "source": [
        "random.shuffle(train_set)\n",
        "random.shuffle(test_set)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnQVSx8GmSk3"
      },
      "source": [
        "print (\"Size dataset full: \"+str(size_dataset_full))\n",
        "print (\"Size training set: \"+str(len(train_set)))\n",
        "print (\"Size test set: \"+str(len(test_set)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size dataset full: 10664\nSize training set: 8531\nSize test set: 2133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IvHkQJ3rbgV"
      },
      "source": [
        "**Excercise 1:** Given a dataset represented as list with instances (as e.g. our `dataset_full` in the RT-polarity dataset) and the size of the test set (e.g. `0.2`) as input, create a function that split the given dataset in training and test sets of the given size. Check your function with our RT-polarity dataset (i.e. `dataset_full`) and `0.2` as inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pFPy0y5rq-h"
      },
      "source": [
        "def get_train_test_split(dataset_full,ratio):\n",
        "  pre_train_set=[]\n",
        "  pre_test_set=[]\n",
        "  # To complete...\n",
        "\n",
        "  return pre_train_set,pre_test_set\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPK7iUyZiNMH"
      },
      "source": [
        "Now we have our dataset split into training and test. However, in many cases we would also need a development set, which can be used to tune our model. To get the development set, we can split the test set in half, and therefore obtain a standard train/dev/test split of 80%/10%/10%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux5ZOHQ7n3iF"
      },
      "source": [
        "original_size_test=len(test_set)\n",
        "size_dev=int(round(original_size_test*0.5,0))\n",
        "list_dev_indices=random.sample(range(original_size_test), size_dev)\n",
        "new_dev_set=[]\n",
        "new_test_set=[]\n",
        "for i,example in enumerate(test_set):\n",
        "  if i in list_dev_indices: new_dev_set.append(example)\n",
        "  else: new_test_set.append(example)\n",
        "new_train_set=train_set\n",
        "random.shuffle(new_train_set)\n",
        "random.shuffle(new_dev_set)\n",
        "random.shuffle(new_test_set)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvnP9l9_pLY_"
      },
      "source": [
        "Our dataset is now split into training, development and test. Let's check some examples from each of the splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugo887VzpWQ5"
      },
      "source": [
        "print (\"TRAINING SET\")\n",
        "print (\"Size training set: \"+str(len(new_train_set)))\n",
        "for example in new_train_set[:3]:\n",
        "  print (example)\n",
        "print (\"    \\n-------\\n\")\n",
        "print (\"DEV SET\")\n",
        "print (\"Size development set: \"+str(len(new_dev_set)))\n",
        "for example in new_dev_set[:3]:\n",
        "  print (example)\n",
        "print (\"    \\n-------\\n\")\n",
        "print (\"TEST SET\")\n",
        "print (\"Size test set: \"+str(len(new_test_set)))\n",
        "for example in new_test_set[:3]:\n",
        "  print (example)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING SET\nSize training set: 8531\n('does point the way for adventurous indian filmmakers toward a crossover into nonethnic markets . ', 1)\n(\"you can sip your vintage wines and watch your merchant ivory productions ; i'll settle for a nice cool glass of iced tea and a jerry bruckheimer flick any day of the week . \", 1)\n(\"the pleasure of read my lips is like seeing a series of perfect black pearls clicking together to form a string . we're drawn in by the dark luster . \", 1)\n    \n-------\n\nDEV SET\nSize development set: 1066\n('i was perplexed to watch it unfold with an astonishing lack of passion or uniqueness . ', 0)\n(\"it's loud and boring ; watching it is like being trapped at a bad rock concert . \", 0)\n('no movement , no yuks , not much of anything . ', 0)\n    \n-------\n\nTEST SET\nSize test set: 1067\n(' if the predictability of bland comfort food appeals to you , then the film is a pleasant enough dish . ', 0)\n('a zombie movie in every sense of the word--mindless , lifeless , meandering , loud , painful , obnoxious . ', 0)\n(' . . . a roller-coaster ride of a movie', 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7m0tAb_-aTq"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "## B) EVALUATION MEASURES\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In this section we will evaluate our linear SVM binary classifier (similar to the one we trained in the previous session) in the RT-polarity dataset. We will first train the model on the training set, and then evaluate it in the test set. To this end, we will use functions from the previous sessions, slightly modified to be more general and cover this case.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRlEgUcvIw_f"
      },
      "source": [
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
        "stopwords.add(\".\")\n",
        "stopwords.add(\",\")\n",
        "stopwords.add(\"--\")\n",
        "stopwords.add(\"``\")\n",
        "\n",
        "# Function taken from Session 1\n",
        "def get_list_tokens(string): # Function to retrieve the list of tokens from a string\n",
        "  sentence_split=nltk.tokenize.sent_tokenize(string)\n",
        "  list_tokens=[]\n",
        "  for sentence in sentence_split:\n",
        "    list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n",
        "    for token in list_tokens_sentence:\n",
        "      list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
        "  return list_tokens\n",
        "\n",
        "# Function taken from Session 2\n",
        "def get_vector_text(list_vocab,string):\n",
        "  vector_text=np.zeros(len(list_vocab))\n",
        "  list_tokens_string=get_list_tokens(string)\n",
        "  for i, word in enumerate(list_vocab):\n",
        "    if word in list_tokens_string:\n",
        "      vector_text[i]=list_tokens_string.count(word)\n",
        "  return vector_text\n",
        "\n",
        "\n",
        "# Functions slightly modified from Session 2\n",
        "\n",
        "def get_vocabulary(training_set, num_features): # Function to retrieve vocabulary\n",
        "  dict_word_frequency={}\n",
        "  for instance in training_set:\n",
        "    sentence_tokens=get_list_tokens(instance[0])\n",
        "    for word in sentence_tokens:\n",
        "      if word in stopwords: continue\n",
        "      if word not in dict_word_frequency: dict_word_frequency[word]=1\n",
        "      else: dict_word_frequency[word]+=1\n",
        "  sorted_list = sorted(dict_word_frequency.items(), key=operator.itemgetter(1), reverse=True)[:num_features]\n",
        "  vocabulary=[]\n",
        "  for word,frequency in sorted_list:\n",
        "    vocabulary.append(word)\n",
        "  return vocabulary\n",
        "\n",
        "def train_svm_classifier(training_set, vocabulary): # Function for training our svm classifier\n",
        "  X_train=[]\n",
        "  Y_train=[]\n",
        "  for instance in training_set:\n",
        "    vector_instance=get_vector_text(vocabulary,instance[0])\n",
        "    X_train.append(vector_instance)\n",
        "    Y_train.append(instance[1])\n",
        "  # Finally, we train the SVM classifier \n",
        "  svm_clf=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
        "  svm_clf.fit(np.asarray(X_train),np.asarray(Y_train))\n",
        "  return svm_clf"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8Rl3pwSnLtW"
      },
      "source": [
        "vocabulary=get_vocabulary(new_train_set, 1000)  # We use the get_vocabulary function to retrieve the vocabulary"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34V7nF4Dr_8w"
      },
      "source": [
        "svm_clf=train_svm_classifier(new_train_set, vocabulary) # We finally use the function to train our SVM classifier. This can take a while..."
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXMERnnPvtZi"
      },
      "source": [
        "We can now test our model with an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FHVHFjHsJgA"
      },
      "source": [
        "print (svm_clf.predict([get_vector_text(vocabulary,\"Fascinating!\")]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vln_3gb4zV_y"
      },
      "source": [
        "Once we have trained our SVM classifier, we can test our model in the training set. To that end, we need to convert the training set in two lists (`X_test` and `Y_test`), similarly as we did with the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwJSvwL4v8eM"
      },
      "source": [
        "X_test=[]\n",
        "Y_test=[]\n",
        "for instance in new_test_set:\n",
        "  vector_instance=get_vector_text(vocabulary,instance[0])\n",
        "  X_test.append(vector_instance)\n",
        "  Y_test.append(instance[1])\n",
        "X_test=np.asarray(X_test)\n",
        "Y_test_gold=np.asarray(Y_test)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OPXPxYBwhv0"
      },
      "source": [
        "We referred to the labels in the test set as `Y_test_gold` to distinguish them from our predictions (*gold standard* makes reference to the ground truth, which are the labels that are known to be correct). Now we can test our model in the test set using `predict` (to obtain the predictions of our model) and `classification_report` (to get the results) from sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nko4wlsWw0Ov"
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9owZU57x8q4"
      },
      "source": [
        "Y_text_predictions=svm_clf.predict(X_test)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JY7G9Rhaw2fh"
      },
      "source": [
        "print(classification_report(Y_test_gold, Y_text_predictions))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n\n           0       0.69      0.75      0.72       519\n           1       0.74      0.67      0.71       548\n\n    accuracy                           0.71      1067\n   macro avg       0.71      0.71      0.71      1067\nweighted avg       0.71      0.71      0.71      1067\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ4WHJugys5S"
      },
      "source": [
        "We can also get the individual accuracy and macro-average precision, recall and F-score individually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU-Ls__RzAsO"
      },
      "source": [
        "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_5_vsFBzJQw"
      },
      "source": [
        "precision=precision_score(Y_test_gold, Y_text_predictions, average='macro')\n",
        "recall=recall_score(Y_test_gold, Y_text_predictions, average='macro')\n",
        "f1=f1_score(Y_test_gold, Y_text_predictions, average='macro')\n",
        "accuracy=accuracy_score(Y_test_gold, Y_text_predictions)\n",
        "\n",
        "print (\"Precision: \"+str(round(precision,3)))\n",
        "print (\"Recall: \"+str(round(recall,3)))\n",
        "print (\"F1-Score: \"+str(round(f1,3)))\n",
        "print (\"Accuracy: \"+str(round(accuracy,3)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.713\nRecall: 0.712\nF1-Score: 0.711\nAccuracy: 0.711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N60XYawkVYx0"
      },
      "source": [
        "To understand better the source of the error made by the model, we can get a confusion matrix (see [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) for more details on confusion matrices in sklearn)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ANdqEL5VjX8"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_2-6vz6Vkq9"
      },
      "source": [
        "print (confusion_matrix(Y_test_gold, Y_text_predictions))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[390 129]\n [179 369]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyUr1Beq0v3a"
      },
      "source": [
        "Depending on your split, your results may vary a bit. As you may have realized, we have not made use of our **development set**! Let's try to tune our model in the development set, as that can help improve our model overall! In the development set we can tune anything we want, from the model to use, to the parameters or features. In our case, let's try to tune the number of features in the test set. We can try with less than 1000 features, which was our vocabulary. For example, let's try with `num_features=250`, `num_features=500`, `num_features=750` and `num_features=1000`. We can then tune our model with respect to these features and optimize it for accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itSiK2uN16l-"
      },
      "source": [
        "# We first get the gold standard labels from the development set\n",
        "\n",
        "Y_dev=[]\n",
        "for instance in new_dev_set:\n",
        "  Y_dev.append(instance[1])\n",
        "Y_dev_gold=np.asarray(Y_dev)\n",
        "\n",
        "# Now we can train our three models with the different number of features, and test each of them in the dev set\n",
        "\n",
        "list_num_features=[250,500,750,1000]\n",
        "best_accuracy_dev=0.0\n",
        "for num_features in list_num_features:\n",
        "  # First, we get the vocabulary from the training set and train our svm classifier\n",
        "  vocabulary=get_vocabulary(new_train_set, num_features)  \n",
        "  svm_clf=train_svm_classifier(new_train_set, vocabulary)\n",
        "  # Then, we transform our dev set into vectors and make the prediction on this set\n",
        "  X_dev=[]\n",
        "  for instance in new_dev_set:\n",
        "    vector_instance=get_vector_text(vocabulary,instance[0])\n",
        "    X_dev.append(vector_instance)\n",
        "  X_dev=np.asarray(X_dev)\n",
        "  Y_dev_predictions=svm_clf.predict(X_dev)\n",
        "  # Finally, we get the accuracy results of the classifier\n",
        "  accuracy_dev=accuracy_score(Y_dev_gold, Y_dev_predictions)\n",
        "  print (\"Accuracy with \"+str(num_features)+\": \"+str(round(accuracy_dev,3)))\n",
        "  if accuracy_dev>=best_accuracy_dev:\n",
        "    best_accuracy_dev=accuracy_dev\n",
        "    best_num_features=num_features\n",
        "    best_vocabulary=vocabulary\n",
        "    best_svm_clf=svm_clf\n",
        "print (\"\\n Best accuracy overall in the dev set is \"+str(round(best_accuracy_dev,3))+\" with \"+str(best_num_features)+\" features.\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with 250: 0.647\n",
            "Accuracy with 500: 0.686\n",
            "Accuracy with 750: 0.691\n",
            "Accuracy with 1000: 0.691\n",
            "\n",
            " Best accuracy overall in the dev set is 0.691 with 1000 features.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd2B3ABz860y"
      },
      "source": [
        "Let's now check the performance (accuracy) of the best model in the test set.\n",
        "\n",
        "**Note:** Not always the best model in the development set leads to the best results on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjz-ZXa48_89"
      },
      "source": [
        "X_test=[]\n",
        "Y_test=[]\n",
        "for instance in new_test_set:\n",
        "  vector_instance=get_vector_text(best_vocabulary,instance[0])\n",
        "  X_test.append(vector_instance)\n",
        "  Y_test.append(instance[1])\n",
        "best_X_test=np.asarray(X_test)\n",
        "Y_test_gold=np.asarray(Y_test)\n",
        "\n",
        "best_Y_text_predictions=best_svm_clf.predict(best_X_test)\n",
        "print(classification_report(Y_test_gold, best_Y_text_predictions))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n\n           0       0.69      0.75      0.72       519\n           1       0.74      0.67      0.71       548\n\n    accuracy                           0.71      1067\n   macro avg       0.71      0.71      0.71      1067\nweighted avg       0.71      0.71      0.71      1067\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFdRoZAr2iTn"
      },
      "source": [
        "**Note:** Please note that we have made use of the test set only once. We haven't evaluated more than one model in the test set. This is important, as any tuning should be done in the test set if we want our method to generalize well and comparable to other models. If we evaluate many times on the test set, we risk overfitting our model to the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXuBJC7b9hW-"
      },
      "source": [
        "**Exercise 2:** Tune the same classifier, this time with `num_features=100`, `num_features=500` and `num_features=1000` and optimize it for macro-average F1-score, instead of accuracy. Test the best-performing classifier in the development set (in terms of F1-score) on the test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ut2-rf7C-94"
      },
      "source": [
        "list_num_features=[100,500,1000]\n",
        "# To complete"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16K6wkXq9wS-"
      },
      "source": [
        "**Exercise (optional):** Think about other elements to tune in the development set. For example, parameters in the SVM (e.g., smaller values of the [C regularization parameter](https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel), more information about the parameters of the SVM [here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)), other vocabulary sizes or features, feature selection methods, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYlpPJlz-NxK"
      },
      "source": [
        "## C) CROSS-VALIDATION\n",
        "\n",
        "In addition to the usual train, development and test splits, there is an alternative that it's called cross-validation. With this technique we use a single set with all our examples, and create several different train/test splits (or train/dev/test). This has the advantage of testing on a wider range of examples (useful especially when your dataset is not very large) but the disadvantage of being computationally more expensive and not easily reproducible.\n",
        "\n",
        "We are going to start with 5-fold validation, i.e., the dataset is split into five parts, which will be used as five different test sets. Let's evaluate our model with 500 features on the full RT-polarity dataset using 5-fold cross-validation.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0-MbK7lJYn8"
      },
      "source": [
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvBT_YX6GzW2"
      },
      "source": [
        "kf = KFold(n_splits=5)\n",
        "random.shuffle(dataset_full)\n",
        "kf.get_n_splits(dataset_full)\n",
        "accuracy_total=0.0\n",
        "for train_index, test_index in kf.split(dataset_full):\n",
        "  train_set_fold=[]\n",
        "  test_set_fold=[]\n",
        "  for i,instance in enumerate(dataset_full):\n",
        "    if i in train_index:\n",
        "      train_set_fold.append(instance)\n",
        "    else:\n",
        "      test_set_fold.append(instance)\n",
        "  vocabulary_fold=get_vocabulary(train_set_fold, 500)\n",
        "  svm_clf_fold=train_svm_classifier(train_set_fold, vocabulary_fold)\n",
        "  X_test_fold=[]\n",
        "  Y_test_fold=[]\n",
        "  for instance in test_set_fold:\n",
        "    vector_instance=get_vector_text(vocabulary_fold,instance[0])\n",
        "    X_test_fold.append(vector_instance)\n",
        "    Y_test_fold.append(instance[1])\n",
        "  Y_test_fold_gold=np.asarray(Y_test_fold)\n",
        "  X_test_fold=np.asarray(X_test_fold)\n",
        "  Y_test_predictions_fold=svm_clf_fold.predict(X_test_fold)\n",
        "  accuracy_fold=accuracy_score(Y_test_fold_gold, Y_test_predictions_fold)\n",
        "  accuracy_total+=accuracy_fold\n",
        "  print (\"Fold completed.\")\n",
        "average_accuracy=accuracy_total/5\n",
        "print (\"\\nAverage Accuracy: \"+str(round(average_accuracy,3)))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold completed.\n",
            "Fold completed.\n",
            "Fold completed.\n",
            "Fold completed.\n",
            "Fold completed.\n",
            "\n",
            "Average Accuracy: 0.68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OhkMwSdMsbS"
      },
      "source": [
        "**Note:** Sklearn contains the [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function, which is very convinent to evaluate our model in a cross-validation setting. However, we cannot use this function when the features depend on the dataset itself, as it is our case in the RT-polarity dataset (the vocabulary depends on the training set)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRP5RfSyN80Q"
      },
      "source": [
        "**Exercise (optional):** Use the `cross_val_score` function from sklearn to evaluate an SVM classifier from the Diabetes dataset (Session 2) using 10-fold cross-validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IENomO9qGyeT"
      },
      "source": [
        "**Exercise 3:** Use 3-fold cross-validation to evaluate the SVM classifier with 1000 features (instead of 500). Print the accuracy of the classifier in every of the three folds, and the overall accuracy at the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp1pR4AWl_fK"
      },
      "source": [
        "# To complete"
      ],
      "execution_count": 32,
      "outputs": []
    }
  ]
}