{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1_Introduction_DataPreprocessing_NLTK_Numpy.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2k3zTKjPSXjm","colab_type":"text"},"source":["*-- Author: Jose Camacho Collados (Cardiff University, Lecturer) --*"]},{"cell_type":"markdown","metadata":{"id":"PG7F15TeVuxb","colab_type":"text"},"source":["This Jupyter notebook is intended to be used as a refresher of Python. It also includes some basic functions which will then be used during the module.\n","\n","Let's start with some basic text (string) preprocessing\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oXTVxIXysM9O","colab_type":"text"},"source":["## TEXT PREPROCESSING WITH NLTK\n","\n","\n","---\n","\n","First, we import the libraries that we need using \"import\".\n","\n","**Note:** All these libraries need to be downloaded beforehand if not using Google Colab. Check their official websites for details on how to install them."]},{"cell_type":"code","metadata":{"id":"-Nv6UldrVuXv","colab_type":"code","colab":{}},"source":["import numpy as np\n","import nltk"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QLX-9_KS2Xer","colab_type":"text"},"source":["We can also download any required dependancies. For example for nltk we will need \"punkt\" for tokenization and \"wordnet\" for lemmatization (you can alternatively download all dependancies with \"nltk.download('all')\"). You only need to do this once.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"vR3p8oif2EC6","colab_type":"code","colab":{}},"source":["nltk.download('punkt')\n","nltk.download('wordnet')\n","#nltk.download('all')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0dlyoorTsYpK","colab_type":"text"},"source":["Text is represented as strings. String can be concatenated using \"+\". For instance, if we are given three sentences we can join them forming a paragraph. Recall that in Python 3 we use the function \"print()\" for printing in console.\n","\n"]},{"cell_type":"code","metadata":{"id":"8wVhvAHAdh3O","colab_type":"code","colab":{}},"source":["sentence1=\"Machine learning is the scientific study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions, relying on patterns and inference instead. \"\n","sentence2=\"It is seen as a subset of artificial intelligence. \"\n","sentence3=\"Machine learning algorithms build a mathematical model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to perform the task. \"\n","paragraph=sentence1+sentence2+sentence3\n","print (paragraph)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1jsBmSJb0QV6","colab_type":"text"},"source":["Let's now preprocess this paragraph. First we need to *tokenize* the text, which means create a list where each element is a word (or a *token*). To do this, we can use the *nltk* library, which is very convinient to deal with text data. "]},{"cell_type":"code","metadata":{"id":"SWgc07DebTQj","colab_type":"code","colab":{}},"source":["list_tokens=nltk.tokenize.word_tokenize(paragraph)\n","print (list_tokens)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BMavgVNz3Vyr","colab_type":"text"},"source":["We can also split the text by sentences, like we had it originally.\n","\n"]},{"cell_type":"code","metadata":{"id":"kudZXXAS1YKU","colab_type":"code","colab":{}},"source":["sentence_split=nltk.tokenize.sent_tokenize(paragraph)\n","print (sentence_split)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i_h_dgzu3xmX","colab_type":"text"},"source":["And now we can tokenize each of the sentence separately and keep it in a new list."]},{"cell_type":"code","metadata":{"id":"xzK6mtos3iZd","colab_type":"code","colab":{}},"source":["list_sentence_tokens=[]\n","for sentence in sentence_split:\n","  list_sentence_tokens.append(nltk.tokenize.word_tokenize(sentence))\n","for sentence_tokens in list_sentence_tokens:\n","  print (sentence_tokens)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1fLFolmO4aKH","colab_type":"text"},"source":["Now that our whole text is tokenized and split into sentences, we can check for example how many sentences contain the word \"learning\""]},{"cell_type":"code","metadata":{"id":"4yLGJDEJ4AbR","colab_type":"code","colab":{}},"source":["count_word=0\n","for sentence_tokens in list_sentence_tokens:\n","  if \"learning\" in sentence_tokens:\n","    count_word+=1\n","print (\"Number of sentences containing 'learning': \"+str(count_word))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VhiV2zX85Rvr","colab_type":"text"},"source":["Sometimes we may want to further preprocess the text. For example, get for each word its *lemma* form, which is its canonical or dictionary form (e.g. \"models\" -> \"model\") or convert each lemma to lowercase (e.g. \"Machine\" -> \"machine\").\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"gRioDJTp5CHA","colab_type":"code","colab":{}},"source":["lemmatizer = nltk.stem.WordNetLemmatizer()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TnJ_G_z03b5Y","colab_type":"code","colab":{}},"source":["list_sentence_lemmas_lower=[]\n","for sentence_tokens in list_sentence_tokens:\n","  list_lemmas=[]\n","  for token in sentence_tokens:\n","    list_lemmas.append(lemmatizer.lemmatize(token).lower())\n","  list_sentence_lemmas_lower.append(list_lemmas)\n","print (list_sentence_lemmas_lower)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1rZJHWDL5XeF","colab_type":"text"},"source":["**Excercise (optional):**\n","Repeat the same text preprocessing procedure but with the Python library [spacy](https://spacy.io/). Spacy is an advanced library for text preprocessing and natural language processing. It tends to provide a better accuracy than NLTK on standard text preprocessing techniques (e.g. lemmatization)."]},{"cell_type":"markdown","metadata":{"id":"5eWRl6uk21zf","colab_type":"text"},"source":["Sometimes it is hard to keep track of all the operations we make. For this, we make use of \"functions\". In our case, it could be interesting to have a function that given a text (string) as input, it gives us a list of lemmas as output."]},{"cell_type":"code","metadata":{"id":"RTSkGzQm3P4O","colab_type":"code","colab":{}},"source":["def get_list_tokens(string):\n","  sentence_split=nltk.tokenize.sent_tokenize(string)\n","  list_tokens=[]\n","  for sentence in sentence_split:\n","    list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n","    for token in list_tokens_sentence:\n","      list_tokens.append(lemmatizer.lemmatize(token).lower())\n","  return list_tokens"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9HzvvsQl5gJs","colab_type":"text"},"source":["Let's check how this function works with our running example."]},{"cell_type":"code","metadata":{"id":"5d6KlmGM5kpm","colab_type":"code","colab":{}},"source":["print (get_list_tokens(paragraph))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m2lVL_Ks5Lno","colab_type":"text"},"source":["Note that with this function you don't keep the information about the individual sentences.\n","\n","**Excercise (optional):** Change slightly this function so that you can keep sentences separated, as in the previous examples."]},{"cell_type":"markdown","metadata":{"id":"E7m0tAb_-aTq","colab_type":"text"},"source":["\n","\n","\n","## VECTOR MANIPULATION WITH NUMPY\n","\n","---\n","\n","\n","For machine learning we always need *vectors* as input, which can be viewed as an array of numbers. For this the *numpy* library is essential, as it provides many useful tools to manipulate vectors. Let's initialize and make some basic operation (e.g. sum) with the vectors.\n","\n"]},{"cell_type":"code","metadata":{"id":"FGgaqwNuUwLd","colab_type":"code","colab":{}},"source":["a=np.zeros(3)\n","b=np.arange(3)\n","print (a)\n","print (b)\n","print (a+b)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PYcKxZ_fVY4n","colab_type":"text"},"source":["Now we are going to create a vector based on the vocabulary of the three sentences we used earlier. Each word corresponds to a position in the vector, where the value is defined by its frequency (i.e. number of occurrences). \n","\n","---\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"XFfJbrfqVOFW","colab_type":"code","colab":{}},"source":["dict_freq_tokens={}\n","for sentence in list_sentence_lemmas_lower:\n","  for token in sentence:\n","    if token in dict_freq_tokens: dict_freq_tokens[token]+=1\n","    else: dict_freq_tokens[token]=1\n","vector_paragraph=np.zeros(len(dict_freq_tokens))\n","list_tokens=list(dict_freq_tokens.keys())\n","print (list_tokens)\n","for i in range(len(list_tokens)):\n","  vector_paragraph[i]=dict_freq_tokens[list_tokens[i]]\n","print (vector_paragraph)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L1KKvt8CQAS4","colab_type":"text"},"source":["We can also compute a vector for each of the sentences, using the same vocabulary (\"list_tokens\")."]},{"cell_type":"code","metadata":{"id":"GGT4sS_hQHtj","colab_type":"code","colab":{}},"source":["dict_freq_tokens={}\n","count_sent=0\n","for sentence in list_sentence_lemmas_lower:\n","  count_sent+=1\n","  for token in sentence:\n","    if token in dict_freq_tokens: dict_freq_tokens[token]+=1\n","    else: dict_freq_tokens[token]=1\n","  for i in range(len(list_tokens)):\n","    token_vocab=list_tokens[i]\n","    if token_vocab in dict_freq_tokens: vector_paragraph[i]=dict_freq_tokens[token_vocab]\n","    else: vector_paragraph[i]=0\n","  print (\"Sentence \"+str(count_sent)+\": \"+str(vector_paragraph))\n","  dict_freq_tokens.clear()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U9YoQ4FwDSSa","colab_type":"text"},"source":["More generally, we may be interested in having a function that, given a pre-defined vocabulary, it computes a frequency vector for any given text.\n","\n","**Exercise 1:** Create a function that takes a vocabulary (as a list of words/lemmas) and a text (string) and outputs its corresponding vector. Test the function with the following inputs: \n","\n","1.   vocabulary=['cat', 'dog', 'machine', 'field']\n","2.   string=\"Machine learning is a field where we study how machines learn.\"\n","\n","Hint: the correct output for this example should be the vector (0,0,2,1)\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"OgcbktDbdaE7","colab_type":"code","colab":{}},"source":["def get_vector_text(list_vocab,string):\n","  #To complete here\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KoThLULaQt_D","colab_type":"text"},"source":["Sometimes we may be interested in how close or similar certain vectors are. For this we can use either the euclidean distance or cosine similarity.\n","\n"]},{"cell_type":"code","metadata":{"id":"tB8_-3bESl0T","colab_type":"code","colab":{}},"source":["#Cosine similarity\n","def cos_sim(a,b):\n","  return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n","\n","#Euclidean distance\n","def euc_dist(a,b):\n","  return np.linalg.norm(a-b)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b4JIcxQvT-Z3","colab_type":"code","colab":{}},"source":["a=np.array([1, 2, 3])\n","b=np.array([10, 21, 32])\n","\n","print (\"Cosine similarity: \"+str(cos_sim(a,b)))\n","print (\"Euclidean distance: \"+str(euc_dist(a,b)))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OE5HJdyuTC1j","colab_type":"text"},"source":["*Reminder:* The cosine similarity takes values between -1 to 1, and only measures the angle between the vectors (the size of the vector is irrelevant)."]},{"cell_type":"markdown","metadata":{"id":"WjxUNQnYUr5K","colab_type":"text"},"source":["**Exercise 2:** With the function defined in Exercise 1 and with the vocabulary ('cat','dog','machine','field'), compute the cosine similarity of the following string pairs:\n","\n","1.   \"Machine learning is a field where we study how machines learn.\" and \"The machine is not working.\"\n","2.   \"My favorite animals are dogs and cats\" and \"The machine is not working.\"\n","3.   \"My favorite animals are dogs and cats\" and \"What can we do with the cat and the dog? The cat is always fighting with the dog.\""]},{"cell_type":"code","metadata":{"id":"EO_4jnUIvWWU","colab_type":"code","colab":{}},"source":["#To complete here\n"],"execution_count":0,"outputs":[]}]}