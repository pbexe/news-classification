{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by making appropriate imports as well as loading the data needed for NLTK and Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "from pprint import pprint\n",
    "from typing import List, Dict\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import spacy\n",
    "# Download the required dataset from NLTK\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import preprocessing, svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# If this fails, please run `python -m spacy download en_core_web_sm`\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two functions can then be defined to load the data from the text files to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(folder: str) -> List[str]:\n",
    "    \"\"\"Load strings from folder of text\n",
    "\n",
    "    Args:\n",
    "        folder (str): The path to the folder to load\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of strings retrieved from text files in the folder\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    # Crawl all subfolders\n",
    "    for root, dirs, files in os.walk(folder, topdown=False):\n",
    "        for name in files:\n",
    "            try:\n",
    "                with open(os.path.join(root, name), \"r\") as fp:\n",
    "                    # Some of the files have non-unicode characters in them so this can fail\n",
    "                    corpus.append(fp.read())\n",
    "            except UnicodeDecodeError as e:\n",
    "                ... # Let the error pass silently\n",
    "                # print(e.__str__(), \"for\", os.path.join(root, name))\n",
    "    return corpus\n",
    "\n",
    "def load_corpuses(folder: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"Load corpuses from sub-folders of specified folder\n",
    "\n",
    "    Args:\n",
    "        folder (str): The parent folder\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[str]]: Dictionary of corpuses\n",
    "    \"\"\"\n",
    "    sub_folders = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        if dirs:\n",
    "            for dir_ in dirs:\n",
    "                sub_folders.append(dir_)\n",
    "\n",
    "    corpuses = {}\n",
    "    for sub_folder in sub_folders:\n",
    "        corpuses[sub_folder] = load_corpus(os.path.join(folder, sub_folder))\n",
    "    return corpuses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then build our dataset using these corpuses. The $x$ vector is made from three features:\n",
    "- Word frequencies\n",
    "- Frequency of named entity types\n",
    "- Weighted word frequencies\n",
    "\n",
    "Once constructed, the data set is shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y =[]\n",
    "\n",
    "corpuses = load_corpuses(\"bbc\")\n",
    "\n",
    "# From the dictionary, generate 2 lists of x and y data\n",
    "for corpus in corpuses:\n",
    "    for story in corpuses[corpus]:\n",
    "        x.append(story)\n",
    "        y.append(corpus)\n",
    "\n",
    "# Shuffle x and y in the same way\n",
    "c = list(zip(x, y))\n",
    "random.shuffle(c)\n",
    "x, y = zip(*c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test and training data are then sampled using a 20:80 split respectively. The $Y$ values are then encoded so that they can be used as labels within the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the 60:20:20 split\n",
    "size_dataset_full=len(x)\n",
    "size_test = int(round(size_dataset_full*0.2,0))\n",
    "size_dev = int(round(size_dataset_full*0.2,0))\n",
    "\n",
    "list_test_indices=random.sample(range(size_dataset_full), size_test)\n",
    "\n",
    "test_x = x[:size_test]\n",
    "test_y = y[:size_test]\n",
    "dev_x = x[size_test+1:size_dev + size_test]\n",
    "dev_y = y[size_test+1:size_dev + size_test]\n",
    "train_x = x[size_dev + size_test:]\n",
    "train_y = y[size_dev + size_test:]\n",
    "\n",
    "# Encode the labels using the labels present in the Y data\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_y)\n",
    "train_y = le.transform(train_y)\n",
    "\n",
    "# Possible for this to fail as a label could be in test that isn't in train\n",
    "test_y = le.transform(test_y)\n",
    "dev_y = le.transform(dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(stories: List[str]) -> List[List[int]]:\n",
    "    \"\"\"Extracts features from a list of strings\n",
    "\n",
    "    Args:\n",
    "        stories (List[str]): Strings to extract features from\n",
    "\n",
    "    Returns:\n",
    "        List[List[int]]: List of vectors which can be used in a model\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit a counter for all of the named entity types in Spacy\n",
    "    entity_types = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "    entity_types.fit(['CARDINAL', 'PERSON', 'GPE', 'MONEY', 'ORG', 'ORDINAL', 'WORK_OF_ART', 'NORP', 'PERCENT', 'DATE', 'LANGUAGE', 'FAC', 'LOC', 'TIME', 'PRODUCT', 'EVENT', 'QUANTITY', 'LAW'])\n",
    "\n",
    "    # Iterate through all of the training data and process it\n",
    "    processed_stories = []\n",
    "    for story in tqdm(stories):\n",
    "        # Apply Spacy NLP to the story\n",
    "        analysed = nlp(story)\n",
    "        processed_stories.append(\n",
    "            # Word grequency matrix\n",
    "            list(vectorizer.transform([story]).toarray()[0]) +\n",
    "            # Named entity type frequency\n",
    "            list(entity_types.transform([tag.label_ for tag in analysed.ents]).toarray()[0]) +\n",
    "            # Weighted word frequency\n",
    "            list(tfid.transform(vectorizer.transform([story])).toarray()[0])\n",
    "        )\n",
    "    return processed_stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define two of the feature extraction methods. `CountVectorizer` builds a vocabulary from the previously loaded training data. `TfidfTransformer` is then built using the matrix provided by `CountVectorizer`.\n",
    "\n",
    "The combination of the vectors resulted in very large $x$ vectors to train on. The best 500 features are selected using the $\\chi^2$ method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1779 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf010553dcc34555bd218938b061c646"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Word frequency counter setup\n",
    "vectorizer = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "# Weighted word frequency counter setup\n",
    "tfid = TfidfTransformer()\n",
    "\n",
    "# Fit the counters to the test data\n",
    "vectorizer.fit(train_x)\n",
    "tfid.fit(vectorizer.transform(train_x))\n",
    "\n",
    "# Perform the pre-processing\n",
    "train_x = feature_extraction(train_x)\n",
    "\n",
    "# Select only the best 500 features\n",
    "get_best=SelectKBest(chi2, k=500).fit(train_x, train_y)\n",
    "train_x_chi = get_best.transform(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM object is constructed. The pipeline includes passing the data through the `StandardScaler` function which \"Standardize \\[sic\\] features by removing the mean and scaling to unit variance\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf=make_pipeline(StandardScaler(), svm.SVC(cache_size=10000, decision_function_shape='ovo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then pass the training data to the SVM to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(cache_size=10000, decision_function_shape='ovo'))])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "svm_clf.fit(train_x_chi, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this model, we can run the training data through it in order to evaluate the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/445 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "112ebd19848c44a4914d1d29d53b5911"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "Y_text_predictions = svm_clf.predict(get_best.transform(feature_extraction(dev_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `classification_report` function allows us to easily generate a report on the success of the SVM by providing known good $Y$ values as well as $Y$ values attained through the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "               precision    recall  f1-score   support\n\n     business       0.95      0.96      0.96       100\nentertainment       0.95      0.95      0.95        66\n     politics       0.99      0.92      0.95        87\n        sport       1.00      0.94      0.97       112\n         tech       0.84      0.96      0.90        80\n\n     accuracy                           0.95       445\n    macro avg       0.95      0.95      0.95       445\n weighted avg       0.95      0.95      0.95       445\n\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(dev_y, Y_text_predictions, target_names=le.inverse_transform(svm_clf.classes_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the SVM, a large amount of preprocessing needs to be done on a string. This has been encapsulated in the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(story: str) -> str:\n",
    "    \"\"\"Gives a genre prediction for a news story\n",
    "\n",
    "    Args:\n",
    "        story (str): The plaintext of the story\n",
    "\n",
    "    Returns:\n",
    "        str: The genre of the story\n",
    "    \"\"\"\n",
    "    return le.inverse_transform(\n",
    "        svm_clf.predict(\n",
    "            get_best.transform(\n",
    "                feature_extraction([story])\n",
    "            )\n",
    "        )\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then try this function with a news story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8c9ea114a0b4788afb5fd810a9f0f8f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'politics'"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "predict(\"\"\"\n",
    "This guide is a concise summary of the main policies being put forward by each party.\n",
    "\n",
    "The policy areas featured in the guide were selected using polling data on what the public consider to be the most important issues facing the country.\n",
    "\n",
    "While some issues such as health and education are the responsibility of the Scottish Parliament, others such as foreign policy and Brexit are decided at the UK parliament at Westminster. You can read more about how devolution works here.\n",
    "\n",
    "More information on how the issues and parties were selected is in our methodology.\n",
    "\n",
    "A full list of parties standing at the election will be published after nominations have closed.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python392jvsc74a57bd0aea754554291bc531c5082a1a77c0df7593cfc14c63ed34a0295729a0fffa55c",
   "display_name": "Python 3.9.2 64-bit ('CMT316-_3fUo8fQ': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "metadata": {
   "interpreter": {
    "hash": "aea754554291bc531c5082a1a77c0df7593cfc14c63ed34a0295729a0fffa55c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}