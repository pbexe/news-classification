{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by making appropriate imports as well as loading the data needed for NLTK and Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "from pprint import pprint\n",
    "from typing import List, Dict\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import spacy\n",
    "# Download the required dataset from NLTK\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import preprocessing, svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# If this fails, please run `python -m spacy download en_core_web_sm`\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two functions can then be defined to load the data from the text files to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(folder: str) -> List[str]:\n",
    "    \"\"\"Load strings from folder of text\n",
    "\n",
    "    Args:\n",
    "        folder (str): The path to the folder to load\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of strings retrieved from text files in the folder\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    for root, dirs, files in os.walk(folder, topdown=False):\n",
    "        for name in files:\n",
    "            try:\n",
    "                with open(os.path.join(root, name), \"r\") as fp:\n",
    "                    corpus.append(fp.read())\n",
    "            except UnicodeDecodeError as e:\n",
    "                ... # Let the error pass silently\n",
    "                # print(e.__str__(), \"for\", os.path.join(root, name))\n",
    "    return corpus\n",
    "\n",
    "def load_corpuses(folder: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"Load corpuses from sub-folders of specified folder\n",
    "\n",
    "    Args:\n",
    "        folder (str): The parent folder\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[str]]: Dictionary of corpuses\n",
    "    \"\"\"\n",
    "    sub_folders = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        if dirs:\n",
    "            for dir_ in dirs:\n",
    "                sub_folders.append(dir_)\n",
    "\n",
    "    corpuses = {}\n",
    "    for sub_folder in sub_folders:\n",
    "        corpuses[sub_folder] = load_corpus(os.path.join(folder, sub_folder))\n",
    "    return corpuses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then build our dataset using these corpuses. The $x$ vector is made from three features:\n",
    "- Word frequencies\n",
    "- Frequency of named entity types\n",
    "- Weighted word frequencies\n",
    "\n",
    "Once constructed, the data set is shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y =[]\n",
    "\n",
    "corpuses = load_corpuses(\"bbc\")\n",
    "\n",
    "for corpus in corpuses:\n",
    "    for story in corpuses[corpus]:\n",
    "        x.append(story)\n",
    "        y.append(corpus)\n",
    "c = list(zip(x, y))\n",
    "\n",
    "random.shuffle(c)\n",
    "\n",
    "x, y = zip(*c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test and training data are then sampled using a 20:80 split respectively. The $Y$ values are then encoded so that they can be used as labels within the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_dataset_full=len(x)\n",
    "size_test=int(round(size_dataset_full*0.2,0))\n",
    "\n",
    "list_test_indices=random.sample(range(size_dataset_full), size_test)\n",
    "\n",
    "test_x = []\n",
    "test_y = []\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "for i,example in enumerate(x):\n",
    "    if i in list_test_indices:\n",
    "        test_x.append(example)\n",
    "        test_y.append(y[i])\n",
    "    else:\n",
    "        train_x.append(example)\n",
    "        train_y.append(y[i])\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_y)\n",
    "train_y = le.transform(train_y)\n",
    "test_y = le.transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(stories: List[str]) -> List[List[int]]:\n",
    "    \"\"\"Extracts features from a list of strings\n",
    "\n",
    "    Args:\n",
    "        stories (List[str]): Strings to extract features from\n",
    "\n",
    "    Returns:\n",
    "        List[List[int]]: List of vectors which can be used in a model\n",
    "    \"\"\"\n",
    "    entity_types = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "    entity_types.fit(['CARDINAL', 'PERSON', 'GPE', 'MONEY', 'ORG', 'ORDINAL', 'WORK_OF_ART', 'NORP', 'PERCENT', 'DATE', 'LANGUAGE', 'FAC', 'LOC', 'TIME', 'PRODUCT', 'EVENT', 'QUANTITY', 'LAW'])\n",
    "    processed_stories = []\n",
    "    for story in tqdm(stories):\n",
    "        analysed = nlp(story)\n",
    "        processed_stories.append(\n",
    "            list(vectorizer.transform([story]).toarray()[0]) + \n",
    "            list(entity_types.transform([tag.label_ for tag in analysed.ents]).toarray()[0]) +\n",
    "            list(tfid.transform(vectorizer.transform([story])).toarray()[0])\n",
    "        )\n",
    "    return processed_stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define two of the feature extraction methods. `CountVectorizer` builds a vocabulary from the previously loaded training data. `TfidfTransformer` is then built using the matrix provided by `CountVectorizer`.\n",
    "\n",
    "The combination of the vectors resulted in very large $x$ vectors to train on. The best 500 features are selected using the $\\chi^2$ method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1779 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce80ee714f2e4bf98746b79f5be98fea"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "tfid = TfidfTransformer()\n",
    "\n",
    "vectorizer.fit(train_x)\n",
    "tfid.fit(vectorizer.transform(train_x))\n",
    "\n",
    "train_x = feature_extraction(train_x)\n",
    "print(type(train_x))\n",
    "\n",
    "get_best=SelectKBest(chi2, k=500).fit(train_x, train_y)\n",
    "train_x_chi = get_best.transform(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[4 4 0 0 1 1 2 2 4 2 4 3 1 1 2 2 2 3 1 1 3 0 1 0 1 2 4 1 4 0 3 2 2 1 0 1 3\n 3 0 0 4 1 0 0 1 1 0 2 4 1 1 2 2 2 2 3 0 4 1 3 2 0 1 1 4 2 3 1 0 4 1 1 4 1\n 2 3 1 3 4 4 1 0 2 4 0 2 1 1 0 4 3 2 1 0 1 0 2 0 3 4 4 4 4 0 3 4 1 4 0 1 3\n 1 2 2 2 1 3 2 2 3 2 4 4 0 3 3 2 1 0 2 0 2 1 4 1 2 1 4 0 4 4 3 4 1 3 2 1 2\n 2 3 2 0 2 1 1 4 1 4 1 2 2 0 2 2 1 0 0 3 2 1 0 0 3 2 2 2 2 2 1 3 2 1 2 2 1\n 3 3 4 4 2 3 3 2 3 0 4 3 0 3 4 4 2 3 3 1 0 3 2 2 4 3 2 0 3 1 4 0 1 4 3 2 0\n 3 1 0 3 4 4 3 1 0 4 1 4 0 4 4 2 2 0 0 2 0 0 1 3 0 2 4 2 0 4 4 0 0 0 4 4 1\n 3 2 4 4 3 1 4 0 3 2 0 3 1 4 4 1 0 3 0 2 4 3 0 3 3 1 0 4 1 0 0 0 1 0 0 2 2\n 3 3 0 0 3 4 0 4 4 3 1 2 0 1 2 0 3 3 3 1 0 2 0 1 4 4 4 0 0 3 2 3 1 4 4 4 3\n 1 0 0 4 2 3 3 4 3 0 2 3 0 2 2 2 4 2 1 3 3 3 1 0 0 1 1 1 3 3 2 3 0 3 2 2 4\n 2 1 4 2 2 2 1 3 0 0 0 3 1 0 1 2 0 3 1 1 2 0 0 3 1 3 4 1 4 4 1 1 3 2 2 3 4\n 3 4 3 2 3 0 0 1 3 3 0 0 0 0 0 0 0 4 3 3 1 3 3 0 4 1 1 1 4 3 0 1 0 0 3 2 1\n 1]\n"
     ]
    }
   ],
   "source": [
    "print(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM object is constructed. The pipeline includes passing the data through the `StandardScaler` function which \"Standardize \\[sic\\] features by removing the mean and scaling to unit variance\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf=make_pipeline(StandardScaler(), svm.SVC(cache_size=10000, decision_function_shape='ovo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then pass the training data to the SVM to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(cache_size=10000, decision_function_shape='ovo'))])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "svm_clf.fit(train_x_chi, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this model, we can run the training data through it in order to evaluate the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/445 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b4e4166751449d49d1509f43e6b3f3a"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "Y_text_predictions = svm_clf.predict(get_best.transform(feature_extraction(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `classification_report` function allows us to easily generate a report on the success of the SVM by providing known good $Y$ values as well as $Y$ values attained through the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "               precision    recall  f1-score   support\n\n     business       0.93      0.97      0.95        97\nentertainment       1.00      0.92      0.96        91\n     politics       0.98      0.96      0.97        89\n        sport       0.99      0.99      0.99        89\n         tech       0.94      1.00      0.97        79\n\n     accuracy                           0.97       445\n    macro avg       0.97      0.97      0.97       445\n weighted avg       0.97      0.97      0.97       445\n\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, Y_text_predictions, target_names=le.inverse_transform(svm_clf.classes_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the SVM, a large amount of preprocessing needs to be done on a string. This has been encapsulated in the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(story: str) -> str:\n",
    "    \"\"\"Gives a genre prediction for a news story\n",
    "\n",
    "    Args:\n",
    "        story (str): The plaintext of the story\n",
    "\n",
    "    Returns:\n",
    "        str: The genre of the story\n",
    "    \"\"\"\n",
    "    return le.inverse_transform(\n",
    "        svm_clf.predict(\n",
    "            get_best.transform(\n",
    "                feature_extraction([story])\n",
    "            )\n",
    "        )\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then try this function with a news story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "263ccef90c70499c9a03eb4bd98dfe29"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'sport'"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "predict(\"\"\"\n",
    "Greene sets sights on world title\n",
    "\n",
    "Maurice Greene aims to wipe out the pain of losing his Olympic 100m title in Athens by winning a fourth World Championship crown this summer.\n",
    "\n",
    "He had to settle for bronze in Greece behind fellow American Justin Gatlin and Francis Obikwelu of Portugal. \"It really hurts to look at that medal. It was my mistake. I lost because of the things I did,\" said Greene, who races in Birmingham on Friday. \"It's never going to happen again. My goal - I'm going to win the worlds.\" Greene crossed the line just 0.02 seconds behind Gatlin, who won in 9.87 seconds in one of the closest and fastest sprints of all time. But Greene believes he lost the race and his title in the semi-finals. \"In my semi-final race, I should have won the race but I was conserving energy. \"That's when Francis Obikwelu came up and I took third because I didn't know he was there. \"I believe that's what put me in lane seven in the final and, while I was in lane seven, I couldn't feel anything in the race.\n",
    "\n",
    "\"I just felt like I was running all alone. \"I believe if I was in the middle of the race I would have been able to react to people that came ahead of me.\" Greene was also denied Olympic gold in the 4x100m men's relay when he could not catch Britain's Mark Lewis-Francis on the final leg. The Kansas star is set to go head-to-head with Lewis-Francis again at Friday's Norwich Union Grand Prix. The pair contest the 60m, the distance over which Greene currently holds the world record of 6.39 seconds. He then has another indoor meeting in France before resuming training for the outdoor season and the task of recapturing his world title in Helsinki in August. Greene believes Gatlin will again prove the biggest threat to his ambitions in Finland. But he also admits he faces more than one rival for the world crown. \"There's always someone else coming. I think when I was coming up I would say there was me and Ato (Boldon) in the young crowd,\" Greene said. \"Now you've got about five or six young guys coming up at the same time.\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python392jvsc74a57bd0aea754554291bc531c5082a1a77c0df7593cfc14c63ed34a0295729a0fffa55c",
   "display_name": "Python 3.9.2 64-bit ('CMT316': pipenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "metadata": {
   "interpreter": {
    "hash": "aea754554291bc531c5082a1a77c0df7593cfc14c63ed34a0295729a0fffa55c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}