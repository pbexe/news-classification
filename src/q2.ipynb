{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "nltk.download(\"all\", quiet=True)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business', 'tech', 'entertainment', 'sport', 'politics']\n",
      "bbc/business\n",
      "bbc/tech\n",
      "bbc/entertainment\n",
      "bbc/sport\n",
      "'utf-8' codec can't decode byte 0xa3 in position 257: invalid start byte for bbc/sport/199.txt\n",
      "bbc/politics\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def load_corpus(folder):\n",
    "    corpus = []\n",
    "    for root, dirs, files in os.walk(folder, topdown=False):\n",
    "        for name in files:\n",
    "            try:\n",
    "                with open(os.path.join(root, name), \"r\") as fp:\n",
    "                    corpus.append(fp.read())\n",
    "            except UnicodeDecodeError as e:\n",
    "                print(e.__str__(), \"for\", os.path.join(root, name))\n",
    "    return corpus\n",
    "\n",
    "def load_corpuses(folder):\n",
    "    sub_folders = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        if dirs:\n",
    "            for dir_ in dirs:\n",
    "                sub_folders.append(dir_)\n",
    "\n",
    "    corpuses = {}\n",
    "    print(sub_folders)\n",
    "    for sub_folder in sub_folders:\n",
    "        print(os.path.join(folder, sub_folder))\n",
    "        corpuses[sub_folder] = load_corpus(os.path.join(folder, sub_folder))\n",
    "    return corpuses\n",
    "\n",
    "corpuses = load_corpuses(\"bbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "tfid = TfidfTransformer()\n",
    "\n",
    "all_ = []\n",
    "for corpus in corpuses:\n",
    "    all_ += corpuses[corpus]\n",
    "\n",
    "vectorizer.fit(all_)\n",
    "tfid.fit(vectorizer.transform(all_))\n",
    "\n",
    "# x = vectorizer.transform(corpuses[\"tech\"])\n",
    "# vectorizer.vocabulary_.get('Sony')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d08281e0b84e86b94c0375db689235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2ffa1f420047bf92364b952105023a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/510 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0106d378d2439dbb641ef58003507c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b6a561721840dea14bf6b22e0dc1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037fd75aa78344468c93d3c8ed30e0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/510 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d19341f15e4bc492ff5bc49fdaab53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/417 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "x = []\n",
    "y =[]\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "entity_types = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "entity_types.fit(['CARDINAL', 'PERSON', 'GPE', 'MONEY', 'ORG', 'ORDINAL', 'WORK_OF_ART', 'NORP', 'PERCENT', 'DATE', 'LANGUAGE', 'FAC', 'LOC', 'TIME', 'PRODUCT', 'EVENT', 'QUANTITY', 'LAW'])\n",
    "for corpus in tqdm(corpuses):\n",
    "    for story in tqdm(corpuses[corpus]):\n",
    "        analysed = nlp(story)\n",
    "        # print(entity_types.transform([tag.label_ for tag in analysed.ents]).toarray()[0])\n",
    "        # print(vectorizer.transform([story]).toarray()[0])\n",
    "        # print(list(tfid.transform(vectorizer.transform([story])).toarray()[0]))\n",
    "        x.append(list(vectorizer.transform([story]).toarray()[0]) + \n",
    "                 list(entity_types.transform([tag.label_ for tag in analysed.ents]).toarray()[0]) +\n",
    "                 list(tfid.transform(vectorizer.transform([story])).toarray()[0])\n",
    "                )\n",
    "        # x.append(tfid.transform(vectorizer.transform([story])).toarray()[0])\n",
    "\n",
    "        y.append(corpus)\n",
    "\n",
    "import random\n",
    "\n",
    "c = list(zip(x, y))\n",
    "\n",
    "random.shuffle(c)\n",
    "\n",
    "x, y = zip(*c)\n",
    "# pprint(x[:3])\n",
    "# pprint(y[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "get_best=SelectKBest(chi2, k=500).fit(x, y)\n",
    "x_chi = get_best.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2224 2224\n"
     ]
    }
   ],
   "source": [
    "print(len(x), len(x_chi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_dataset_full=len(x_chi)\n",
    "size_test=int(round(size_dataset_full*0.2,0))\n",
    "\n",
    "list_test_indices=random.sample(range(size_dataset_full), size_test)\n",
    "\n",
    "test_x = []\n",
    "test_y = []\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "for i,example in enumerate(x_chi):\n",
    "  if i in list_test_indices:\n",
    "      test_x.append(example)\n",
    "      test_y.append(y[i])\n",
    "  else:\n",
    "      train_x.append(example)\n",
    "      train_y.append(y[i])\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_y + test_y)\n",
    "train_y = le.transform(train_y)\n",
    "test_y = le.transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 4 3 2 1 0 4 3 2 4 3 0 3 1 1 2 0 4 3 0 0 1 0 3 4 4 2 1 4 2 4 0 0 0 0 0\n",
      " 0 3 3 4 2 4 2 4 1 1 0 0 0 3 4 1 2 3 2 0 2 4 1 2 0 3 2 3 4 3 0 3 0 4 1 2 3\n",
      " 0 0 3 2 3 2 1 1 0 3 0 1 4 2 0 3 2 3 1 1 0 4 3 2 0 0 0 2 1 1 2 4 2 0 1 0 0\n",
      " 2 0 4 4 4 2 1 1 3 3 3 0 0 1 3 4 3 3 1 1 4 4 4 0 1 4 1 1 2 0 2 3 3 2 4 3 2\n",
      " 0 4 3 1 3 2 3 2 3 0 0 1 3 2 0 3 1 0 2 3 2 0 2 2 1 0 1 3 4 3 3 1 0 2 0 2 1\n",
      " 3 4 0 1 1 0 4 4 2 3 2 1 0 1 3 2 0 3 3 1 3 1 0 0 4 1 0 3 2 0 2 3 0 4 0 4 3\n",
      " 1 0 0 0 2 3 1 1 1 3 1 1 4 3 2 3 1 2 3 3 3 0 4 3 2 2 2 4 3 0 2 2 2 0 3 0 0\n",
      " 3 4 3 1 2 0 2 1 0 0 3 1 4 2 2 4 0 2 0 3 3 1 3 2 3 4 4 0 1 0 1 4 4 3 0 0 0\n",
      " 3 4 3 0 2 1 0 3 2 2 2 2 3 0 2 0 3 1 1 4 4 4 3 0 2 1 0 3 1 4 3 1 0 1 4 4 3\n",
      " 1 3 1 3 1 3 0 1 2 0 0 4 0 4 1 4 4 0 0 3 3 0 0 2 1 3 0 0 2 0 2 2 2 1 1 2 3\n",
      " 1 2 3 1 3 3 3 3 4 0 4 4 1 2 0 1 0 3 0 4 1 4 1 0 2 2 0 0 2 4 0 4 0 0 2 3 0\n",
      " 2 3 1 3 2 1 3 1 3 0 0 4 1 4 0 0 1 3 0 1 1 3 2 0 1 3 4 2 4 1 0 2 0 0 0 3 0\n",
      " 4]\n"
     ]
    }
   ],
   "source": [
    "print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "\n",
    "svm_clf=make_pipeline(StandardScaler(), svm.SVC(cache_size=10000, decision_function_shape='ovo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(cache_size=10000, decision_function_shape='ovo'))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_text_predictions = svm_clf.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94       114\n",
      "           1       1.00      0.89      0.94        83\n",
      "           2       0.99      0.93      0.96        81\n",
      "           3       0.97      0.97      0.97        99\n",
      "           4       0.80      0.99      0.88        68\n",
      "\n",
      "    accuracy                           0.94       445\n",
      "   macro avg       0.94      0.94      0.94       445\n",
      "weighted avg       0.95      0.94      0.94       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y, Y_text_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sport'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(story):\n",
    "    return le.inverse_transform(\n",
    "        svm_clf.predict(\n",
    "            get_best.transform(\n",
    "                [\n",
    "                    list(vectorizer.transform([story]).toarray()[0]) + \n",
    "                    list(entity_types.transform([tag.label_ for tag in nlp(story).ents]).toarray()[0]) +\n",
    "                    list(tfid.transform(vectorizer.transform([story])).toarray()[0])\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    )[0]\n",
    "\n",
    "\n",
    "predict(\"\"\"\n",
    "Greene sets sights on world title\n",
    "\n",
    "Maurice Greene aims to wipe out the pain of losing his Olympic 100m title in Athens by winning a fourth World Championship crown this summer.\n",
    "\n",
    "He had to settle for bronze in Greece behind fellow American Justin Gatlin and Francis Obikwelu of Portugal. \"It really hurts to look at that medal. It was my mistake. I lost because of the things I did,\" said Greene, who races in Birmingham on Friday. \"It's never going to happen again. My goal - I'm going to win the worlds.\" Greene crossed the line just 0.02 seconds behind Gatlin, who won in 9.87 seconds in one of the closest and fastest sprints of all time. But Greene believes he lost the race and his title in the semi-finals. \"In my semi-final race, I should have won the race but I was conserving energy. \"That's when Francis Obikwelu came up and I took third because I didn't know he was there. \"I believe that's what put me in lane seven in the final and, while I was in lane seven, I couldn't feel anything in the race.\n",
    "\n",
    "\"I just felt like I was running all alone. \"I believe if I was in the middle of the race I would have been able to react to people that came ahead of me.\" Greene was also denied Olympic gold in the 4x100m men's relay when he could not catch Britain's Mark Lewis-Francis on the final leg. The Kansas star is set to go head-to-head with Lewis-Francis again at Friday's Norwich Union Grand Prix. The pair contest the 60m, the distance over which Greene currently holds the world record of 6.39 seconds. He then has another indoor meeting in France before resuming training for the outdoor season and the task of recapturing his world title in Helsinki in August. Greene believes Gatlin will again prove the biggest threat to his ambitions in Finland. But he also admits he faces more than one rival for the world crown. \"There's always someone else coming. I think when I was coming up I would say there was me and Ato (Boldon) in the young crowd,\" Greene said. \"Now you've got about five or six young guys coming up at the same time.\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "aea754554291bc531c5082a1a77c0df7593cfc14c63ed34a0295729a0fffa55c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
